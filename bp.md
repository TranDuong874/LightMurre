# ‚≠ê **MULTIVIEW CONSISTENCY BLUEPRINT**

### Using only:

‚úî rgb.png
‚úî intrinsic.npy (K)
‚úî extrinsic.npy (E, world‚Üícam)
‚úî predicted depth (DepthAnything + Œîz)

---

# üü¶ **0. NOTATION**

Let:

* view **i** be the source
* view **j** be the target

Inputs per-view:

```
image_i        (H, W, 3)
depth_pred_i   (H, W)
intrinsic_i    (3, 3)
extrinsic_i    (4, 4)   # world ‚Üí cam
```

---

# üü© **1. SAMPLE A PIXEL FROM VIEW i**

Pick a random pixel in view i:

```
(u_i, v_i)
```

Get predicted depth:

```
d_i = depth_pred_i[v_i, u_i]
```

---

# üü© **2. LIFT PIXEL IN VIEW i ‚Üí 3D POINT X·µ¢ (camera space)**

Backproject using intrinsics:

[
X^{cam}_i = d_i \cdot K^{-1}_i [u_i, v_i, 1]^T
]

This gives a 3D point **in camera-i coordinates**.

---

# üü© **3. TRANSFORM X·µ¢ TO WORLD COORDINATES**

Extrinsics are world‚Üícam, so invert:

[
X_i = E_i^{-1} X^{cam}_i
]

This is the world-space 3D point reconstructed **from view i**.

---

# üü© **4. PROJECT X·µ¢ INTO VIEW j ‚Üí FIND PIXEL u‚±º**

Use camera j extrinsics + intrinsics:

[
X_j^{cam} = E_j X_i
]

[
u_j = K_j \frac{X^{cam}_j}{Z^{cam}_j}
]

This gives the corresponding pixel in view j:

```
(u_j, v_j)
```

If (u_j, v_j) is outside image ‚Üí skip this pixel.

---

# üü© **5. READ DEPTH IN VIEW j AT THAT LOCATION**

Use bilinear sampling:

```
d_j = depth_pred_j(v_j, u_j)
```

---

# üü© **6. LIFT PIXEL IN VIEW j ‚Üí 3D POINT X‚±º (camera space)**

[
X^{cam}_j = d_j \cdot K^{-1}_j [u_j, v_j, 1]^T
]

---

# üü© **7. TRANSFORM X‚±º TO WORLD COORDINATES**

[
X_j = E_j^{-1} X^{cam}_j
]

This is the world-space 3D point reconstructed **from view j**.

---

# üü© **8. MULTIVIEW CONSISTENCY LOSS**

[
L_{mv} = |X_i - X_j|_2
]

If depth predictions are inconsistent,
(X_i) and (X_j) are **far apart** ‚Üí large loss.

Training updates Œîz to reduce this.

---

# üü© **9. GRADIENT FLOWS TO Œîz**

Because:

* (X_i) depends on depth_i ‚Üí depends on Œîz_i
* (X_j) depends on depth_j ‚Üí depends on Œîz_j

The loss backpropagates into Œîz network parameters.

The model learns to:

* correct scale
* correct shape
* enforce depth consistency across views

---

# ‚≠ê **FINAL BLUEPRINT (compact form)**

```
for each view i:
    sample u_i,v_i
    d_i = depth_pred_i[u_i,v_i]
    X_i_cam = backproject(u_i,v_i,d_i,K_i)
    X_i = cam_to_world(E_i, X_i_cam)

    for each other view j:
        u_j, v_j = project_to_image(K_j, E_j, X_i)
        if not inside_image(u_j,v_j): continue

        d_j = depth_pred_j(u_j,v_j)    # bilinear
        X_j_cam = backproject(u_j,v_j,d_j,K_j)
        X_j = cam_to_world(E_j, X_j_cam)

        L_mv += || X_i - X_j ||‚ÇÇ
```

---

# ‚≠ê WHY THIS WORKS

Because the only possible way to minimize:

[
|X_i - X_j|
]

is for the model to adjust depth (via Œîz) so that:

### ‚Üí 3D reconstructed from both views matches

### ‚Üí depth is consistent in world space

### ‚Üí all point clouds align

### ‚Üí monocular depth loses scale errors

### ‚Üí depth improves without GT depth
