{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84423ed",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a21a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43e2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6944dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channel=7, out_channel=1, base=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_channel, base)\n",
    "        self.enc2 = ConvBlock(base, base*2)\n",
    "        self.enc3 = ConvBlock(base*2, base*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base*4, base*2)\n",
    "        self.dec1 = ConvBlock(base*2, base)\n",
    "        self.out = nn.Conv2d(base, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        \n",
    "        d2 = self.up2(e3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.out(d1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58c7b3",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbfd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_gtav_exr.py\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import OpenEXR\n",
    "import Imath\n",
    "\n",
    "\n",
    "def load_exr_depth(path):\n",
    "    exr = OpenEXR.InputFile(path)\n",
    "    dw = exr.header()['dataWindow']\n",
    "    width  = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "\n",
    "    channel = 'Z' if 'Z' in exr.header()['channels'] else 'Y'\n",
    "    pt = Imath.PixelType(Imath.PixelType.FLOAT)\n",
    "    depth_str = exr.channel(channel, pt)\n",
    "\n",
    "    depth = np.frombuffer(depth_str, dtype=np.float32)\n",
    "    return depth.reshape((height, width))\n",
    "\n",
    "\n",
    "def load_pose_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    extr = np.array(meta[\"extrinsic\"], dtype=np.float32)\n",
    "    fx, fy = meta[\"f_x\"], meta[\"f_y\"]\n",
    "    cx, cy = meta[\"c_x\"], meta[\"c_y\"]\n",
    "\n",
    "    K = np.array([\n",
    "        [fx, 0,  cx],\n",
    "        [0,  fy, cy],\n",
    "        [0,   0,  1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Fix left-handed coordinate issues\n",
    "    R = extr[:3, :3]\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R[:, 2] *= -1\n",
    "        extr[:3, :3] = R\n",
    "\n",
    "    return extr, K\n",
    "\n",
    "\n",
    "class GTAVEXRDataset(Dataset):\n",
    "    def __init__(self, root, n_views=3, max_stride=3):\n",
    "        assert n_views % 2 == 1, \"n_views must be odd: 3,5,7...\"\n",
    "        self.n_views = n_views\n",
    "        self.max_stride = max_stride\n",
    "        self.root = root\n",
    "\n",
    "        self.scenes = []\n",
    "\n",
    "        scene_ids = sorted(os.listdir(root))\n",
    "        for sid in scene_ids:\n",
    "            spath = os.path.join(root, sid)\n",
    "            if not os.path.isdir(spath):\n",
    "                continue\n",
    "\n",
    "            imgs   = sorted(os.listdir(os.path.join(spath, \"images\")))\n",
    "            depths = sorted(os.listdir(os.path.join(spath, \"depths\")))\n",
    "            poses  = sorted(os.listdir(os.path.join(spath, \"poses\")))\n",
    "\n",
    "            self.scenes.append({\n",
    "                \"img\":   [os.path.join(spath, \"images\", f) for f in imgs],\n",
    "                \"depth\": [os.path.join(spath, \"depths\", f) for f in depths],\n",
    "                \"pose\":  [os.path.join(spath, \"poses\", f) for f in poses],\n",
    "                \"len\": len(imgs),\n",
    "                \"root\": spath\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(s[\"len\"] for s in self.scenes)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Generalized symmetric sampling around center (Option 1)\n",
    "    # ----------------------------------------------------------\n",
    "    def sample_views(self, L):\n",
    "        \"\"\"\n",
    "        Returns sorted list of n_views indices:\n",
    "        symmetric around a random center.\n",
    "        \"\"\"\n",
    "        half = self.n_views // 2\n",
    "        # ensure center can shift both sides\n",
    "        c = random.randint(half, L - half - 1)\n",
    "\n",
    "        views = [c]\n",
    "\n",
    "        for h in range(1, half + 1):\n",
    "            # random stride for left/right\n",
    "            k = random.randint(1, self.max_stride)\n",
    "\n",
    "            left  = max(0,     c - k)\n",
    "            right = min(L - 1, c + k)\n",
    "\n",
    "            views.append(left)\n",
    "            views.append(right)\n",
    "\n",
    "        views = sorted(views)[:self.n_views]   # ensure correct count\n",
    "        return views\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # __getitem__\n",
    "    # ----------------------------------------------------------\n",
    "    def __getitem__(self, _):\n",
    "        scene = random.choice(self.scenes)\n",
    "        L = scene[\"len\"]\n",
    "\n",
    "        # pick generalized multi-view indices\n",
    "        idxs = self.sample_views(L)   # e.g. [c-k2, c-k1, c, c+k1, c+k2] for 5 views\n",
    "\n",
    "        # Load RGB\n",
    "        def load_rgb(path):\n",
    "            img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            return img.astype(np.float32) / 255.0\n",
    "\n",
    "        rgb = np.stack([load_rgb(scene[\"img\"][v]) for v in idxs])   # (V,H,W,3)\n",
    "        depth = np.stack([load_exr_depth(scene[\"depth\"][v]) for v in idxs])   # (V,H,W)\n",
    "\n",
    "        # Load extrinsics/intrinsics\n",
    "        extrinsics = []\n",
    "        intrinsics = []\n",
    "\n",
    "        for v in idxs:\n",
    "            extr, K = load_pose_json(scene[\"pose\"][v])\n",
    "            extrinsics.append(extr)\n",
    "            intrinsics.append(K)\n",
    "\n",
    "        extrinsics = np.stack(extrinsics)   # (V,4,4)\n",
    "        intrinsics = np.stack(intrinsics)   # (V,3,3)\n",
    "\n",
    "        return {\n",
    "            \"rgb\": torch.from_numpy(rgb).permute(0, 3, 1, 2),  # (V,3,H,W)\n",
    "            \"depth\": torch.from_numpy(depth).float(),         # (V,H,W)\n",
    "            \"extrinsic\": torch.from_numpy(extrinsics).float(),# (V,4,4)\n",
    "            \"intrinsic\": torch.from_numpy(intrinsics).float() # (V,3,3)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3d564",
   "metadata": {},
   "source": [
    "# Cache Generation\n",
    "\n",
    "Pre-compute all DepthAnything predictions and save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77bb8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def generate_depth_cache(dataset_root, cache_root, depth_anything_model, batch_size=8):\n",
    "    \"\"\"\n",
    "    Pre-compute ALL DepthAnything predictions and save to cache.\n",
    "    Processes images in batches for faster inference.\n",
    "    \"\"\"\n",
    "    dataset_root = Path(dataset_root)\n",
    "    cache_root = Path(cache_root)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"DepthAnything Cache Generation (BATCHED)\".center(70))\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Dataset:    {dataset_root}\")\n",
    "    print(f\"Cache:      {cache_root}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    scenes = sorted([d for d in dataset_root.iterdir() if d.is_dir()])\n",
    "    \n",
    "    total_images = 0\n",
    "    total_cached = 0\n",
    "    \n",
    "    for scene in tqdm(scenes, desc=\"Scenes\"):\n",
    "        scene_name = scene.name\n",
    "        image_dir = scene / \"images\"\n",
    "        cache_dir = cache_root / scene_name / \"depths\"\n",
    "        \n",
    "        # Create cache directory\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get all images\n",
    "        image_files = sorted(image_dir.glob(\"*.png\"))\n",
    "        \n",
    "        # Separate already cached from to-process\n",
    "        to_process = []\n",
    "        to_process_paths = []\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            cache_file = cache_dir / f\"{img_file.stem}.npy\"\n",
    "            \n",
    "            if cache_file.exists():\n",
    "                total_cached += 1\n",
    "            else:\n",
    "                to_process.append(img_file)\n",
    "                to_process_paths.append(cache_file)\n",
    "        \n",
    "        # Process in batches\n",
    "        if len(to_process) > 0:\n",
    "            for i in tqdm(range(0, len(to_process), batch_size), \n",
    "                         desc=f\"  {scene_name}\", leave=False):\n",
    "                batch_files = to_process[i:i+batch_size]\n",
    "                batch_cache_paths = to_process_paths[i:i+batch_size]\n",
    "                \n",
    "                # Load batch of images\n",
    "                batch_images = [Image.open(f).convert(\"RGB\") for f in batch_files]\n",
    "                \n",
    "                # Run DepthAnything on batch\n",
    "                with torch.no_grad():\n",
    "                    results = depth_anything_model(batch_images)\n",
    "                \n",
    "                # Handle both single result and list of results\n",
    "                if not isinstance(results, list):\n",
    "                    results = [results]\n",
    "                \n",
    "                # Process and save each result\n",
    "                for result, cache_path in zip(results, batch_cache_paths):\n",
    "                    depth_pred = result[\"depth\"]\n",
    "                    if isinstance(depth_pred, Image.Image):\n",
    "                        depth_pred = np.array(depth_pred)\n",
    "                    \n",
    "                    # Normalize to [0, 1]\n",
    "                    depth_array = depth_pred.astype(np.float32)\n",
    "                    depth_min = depth_array.min()\n",
    "                    depth_max = depth_array.max()\n",
    "                    if depth_max > depth_min:\n",
    "                        depth_normalized = (depth_array - depth_min) / (depth_max - depth_min)\n",
    "                    else:\n",
    "                        depth_normalized = np.zeros_like(depth_array)\n",
    "                    \n",
    "                    # Save to cache\n",
    "                    np.save(cache_path, depth_normalized)\n",
    "                    total_images += 1\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Cache Generation Complete\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Newly cached:    {total_images} images\")\n",
    "    print(f\"Already cached:  {total_cached} images\")\n",
    "    print(f\"Total:           {total_images + total_cached} images\")\n",
    "    print(f\"Cache location:  {cache_root}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return total_images + total_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19433c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTAVCachedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    GTAV dataset that loads pre-cached DepthAnything predictions.\n",
    "    Cache MUST exist - run cache generation first!\n",
    "    \"\"\"\n",
    "    def __init__(self, root, cache_root, n_views=3, max_stride=3):\n",
    "        assert n_views % 2 == 1, \"n_views must be odd: 3,5,7...\"\n",
    "        self.n_views = n_views\n",
    "        self.max_stride = max_stride\n",
    "        self.root = root\n",
    "        self.cache_root = cache_root\n",
    "\n",
    "        self.scenes = []\n",
    "\n",
    "        scene_ids = sorted(os.listdir(root))\n",
    "        for sid in scene_ids:\n",
    "            spath = os.path.join(root, sid)\n",
    "            if not os.path.isdir(spath):\n",
    "                continue\n",
    "\n",
    "            imgs = sorted(os.listdir(os.path.join(spath, \"images\")))\n",
    "            depths = sorted(os.listdir(os.path.join(spath, \"depths\")))\n",
    "            poses = sorted(os.listdir(os.path.join(spath, \"poses\")))\n",
    "            \n",
    "            # Check if cache exists for this scene\n",
    "            cache_scene = os.path.join(cache_root, sid, \"depths\")\n",
    "            if not os.path.exists(cache_scene):\n",
    "                print(f\"‚ö†Ô∏è  Skipping scene {sid} - no cache found!\")\n",
    "                continue\n",
    "            \n",
    "            cached_depths = sorted(os.listdir(cache_scene))\n",
    "\n",
    "            self.scenes.append({\n",
    "                \"img\": [os.path.join(spath, \"images\", f) for f in imgs],\n",
    "                \"depth\": [os.path.join(spath, \"depths\", f) for f in depths],\n",
    "                \"depth_cached\": [os.path.join(cache_scene, f) for f in cached_depths],\n",
    "                \"pose\": [os.path.join(spath, \"poses\", f) for f in poses],\n",
    "                \"len\": len(imgs),\n",
    "                \"root\": spath\n",
    "            })\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(self.scenes)} scenes with cache\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(s[\"len\"] for s in self.scenes)\n",
    "\n",
    "    def sample_views(self, L):\n",
    "        \"\"\"Returns sorted list of n_views indices symmetric around a random center.\"\"\"\n",
    "        half = self.n_views // 2\n",
    "        c = random.randint(half, L - half - 1)\n",
    "\n",
    "        views = [c]\n",
    "        for h in range(1, half + 1):\n",
    "            k = random.randint(1, self.max_stride)\n",
    "            left = max(0, c - k)\n",
    "            right = min(L - 1, c + k)\n",
    "            views.append(left)\n",
    "            views.append(right)\n",
    "\n",
    "        views = sorted(views)[:self.n_views]\n",
    "        return views\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        scene = random.choice(self.scenes)\n",
    "        L = scene[\"len\"]\n",
    "        idxs = self.sample_views(L)\n",
    "\n",
    "        # Load RGB\n",
    "        def load_rgb(path):\n",
    "            img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            return img.astype(np.float32) / 255.0\n",
    "\n",
    "        rgb = np.stack([load_rgb(scene[\"img\"][v]) for v in idxs])\n",
    "        depth = np.stack([load_exr_depth(scene[\"depth\"][v]) for v in idxs])\n",
    "        \n",
    "        # Load cached DepthAnything predictions\n",
    "        depth_init = np.stack([np.load(scene[\"depth_cached\"][v]) for v in idxs])\n",
    "\n",
    "        # Load extrinsics/intrinsics\n",
    "        extrinsics = []\n",
    "        intrinsics = []\n",
    "\n",
    "        for v in idxs:\n",
    "            extr, K = load_pose_json(scene[\"pose\"][v])\n",
    "            extrinsics.append(extr)\n",
    "            intrinsics.append(K)\n",
    "\n",
    "        extrinsics = np.stack(extrinsics)\n",
    "        intrinsics = np.stack(intrinsics)\n",
    "\n",
    "        return {\n",
    "            \"rgb\": torch.from_numpy(rgb).permute(0, 3, 1, 2),  # (V,3,H,W)\n",
    "            \"depth\": torch.from_numpy(depth).float(),  # (V,H,W) - GT depth\n",
    "            \"depth_init\": torch.from_numpy(depth_init).float(),  # (V,H,W) - Cached DepthAnything\n",
    "            \"extrinsic\": torch.from_numpy(extrinsics).float(),  # (V,4,4)\n",
    "            \"intrinsic\": torch.from_numpy(intrinsics).float()  # (V,3,3)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482cf74",
   "metadata": {},
   "source": [
    "# Step 1: Generate Cache\n",
    "\n",
    "Load DepthAnything model and generate cache for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96e0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cache complete: 300/300 images\n",
      "‚úÖ Using existing cache - no generation needed!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "dataset_root = \"dataset/GTAV_540_mini_test\"\n",
    "cache_root = \"dataset/GTAV_540_mini_test_cache\"\n",
    "\n",
    "# Check if cache exists\n",
    "cache_path = Path(cache_root)\n",
    "needs_caching = False\n",
    "\n",
    "if not cache_path.exists():\n",
    "    needs_caching = True\n",
    "    print(f\"‚ö†Ô∏è  Cache directory not found: {cache_root}\")\n",
    "else:\n",
    "    # Count cached vs total images\n",
    "    dataset_path = Path(dataset_root)\n",
    "    scenes = sorted([d for d in dataset_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    total_images = 0\n",
    "    cached_images = 0\n",
    "    \n",
    "    for scene in scenes:\n",
    "        images = list((scene / \"images\").glob(\"*.png\"))\n",
    "        total_images += len(images)\n",
    "        \n",
    "        cache_scene = cache_path / scene.name / \"depths\"\n",
    "        if cache_scene.exists():\n",
    "            cached_images += len(list(cache_scene.glob(\"*.npy\")))\n",
    "    \n",
    "    if cached_images < total_images:\n",
    "        needs_caching = True\n",
    "        print(f\"‚ö†Ô∏è  Incomplete cache: {cached_images}/{total_images} images\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Cache complete: {cached_images}/{total_images} images\")\n",
    "\n",
    "# Generate cache if needed\n",
    "if needs_caching:\n",
    "    print(\"\\nüîÑ Generating depth cache...\")\n",
    "    \n",
    "    # Load DepthAnything model\n",
    "    if 'depth_anything' not in dir():\n",
    "        from transformers import pipeline\n",
    "        print(\"Loading DepthAnything model...\")\n",
    "        depth_anything = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "        print(\"‚úì Model loaded!\")\n",
    "    \n",
    "    # Generate cache\n",
    "    cache_count = generate_depth_cache(\n",
    "        dataset_root=dataset_root,\n",
    "        cache_root=cache_root,\n",
    "        depth_anything_model=depth_anything,\n",
    "        batch_size=8  # Process 8 images at a time for speed\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Cache generation complete! {cache_count} depth maps ready.\")\n",
    "else:\n",
    "    print(\"‚úÖ Using existing cache - no generation needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed6f12",
   "metadata": {},
   "source": [
    "# Step 2: Load Dataset with Cache\n",
    "\n",
    "Initialize dataset that uses cached depth predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4e8067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 3 scenes with cache\n",
      "RGB: torch.Size([3, 5, 3, 540, 960])\n",
      "Depth (GT): torch.Size([3, 5, 540, 960])\n",
      "Depth (Init): torch.Size([3, 5, 540, 960])\n",
      "Extrinsic: torch.Size([3, 5, 4, 4])\n",
      "Intrinsic: torch.Size([3, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset with cache\n",
    "dataset = GTAVCachedDataset(\n",
    "    root=dataset_root,\n",
    "    cache_root=cache_root,\n",
    "    max_stride=3,\n",
    "    n_views=5\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Test 1 batch\n",
    "test_sample = None\n",
    "for batch in dataloader:\n",
    "    test_sample = batch\n",
    "    print(\"RGB:\", batch[\"rgb\"].shape)  # (B, V, 3, H, W)\n",
    "    print(\"Depth (GT):\", batch[\"depth\"].shape)  # (B, V, H, W)\n",
    "    print(\"Depth (Init):\", batch[\"depth_init\"].shape)  # (B, V, H, W) - Cached!\n",
    "    print(\"Extrinsic:\", batch[\"extrinsic\"].shape)  # (B, V, 4, 4)\n",
    "    print(\"Intrinsic:\", batch[\"intrinsic\"].shape)  # (B, V, 3, 3)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f811594",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01c270a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split: Train=270, Val=30\n",
      "Train loader: 90 batches\n",
      "Val loader: 10 batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split dataset: 90% train, 10% validation\n",
    "train_ratio = 0.9\n",
    "val_ratio = 0.1\n",
    "seed = 42\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "print(f\"Dataset split: Train={len(train_dataset)}, Val={len(val_dataset)}\")\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96794dbb",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275488c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([540, 960, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ray_dirs(intrinsics, W, H):\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        indexing='xy'\n",
    "    ) \n",
    "\n",
    "    dirs = torch.stack([\n",
    "        (i - intrinsics[0, 2]) / intrinsics[0, 0], # [(X_i) - (fx)]/ (B, cx) \n",
    "        (j - intrinsics[1, 2]) / intrinsics[1, 1],\n",
    "        torch.ones_like(i)\n",
    "    ], -1) \n",
    "\n",
    "    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "    return dirs  # (W,H,[dir_x, dir_y, dir_z])\n",
    "\n",
    "# Test\n",
    "W, H = test_sample['rgb'].shape[-1], test_sample['rgb'].shape[-2]\n",
    "calculate_ray_dirs(test_sample[\"intrinsic\"][0,1], W, H).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bcd393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([540, 960, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def project_depth_to_camera_3d(depth_map, ray_dirs):\n",
    "    return depth_map[..., None] * ray_dirs # return [X, Y, Z] 3D points in camera space\n",
    "\n",
    "def project_3d_to_camera_2d(points_3d, intrinsic):\n",
    "    \"\"\"\n",
    "    points_3d: (..., 3) tensor of 3D points in camera space\n",
    "    intrinsic: (3, 3) camera intrinsic matrix\n",
    "    Returns:\n",
    "        points_2d: (..., 2) tensor of 2D pixel coordinates\n",
    "    \"\"\"\n",
    "    fx = intrinsic[0, 0]\n",
    "    fy = intrinsic[1, 1]\n",
    "    cx = intrinsic[0, 2]\n",
    "    cy = intrinsic[1, 2]\n",
    "\n",
    "    x = points_3d[..., 0]\n",
    "    y = points_3d[..., 1]\n",
    "    z = points_3d[..., 2].clamp(min=1e-6)  # Prevent division by zero\n",
    "\n",
    "    u = fx * (x / z) + cx\n",
    "    v = fy * (y / z) + cy\n",
    "\n",
    "    return torch.stack([u, v], dim=-1)\n",
    "\n",
    "# Test\n",
    "project_depth_to_camera_3d(\n",
    "    test_sample['depth'][0,1], \n",
    "    calculate_ray_dirs(test_sample[\"intrinsic\"][0,1], W, H)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b87cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_world(cam_point_3d, extrinsic):\n",
    "    \"\"\"\n",
    "    cam_point_3d: (H, W, 3) camera-space 3D points\n",
    "    extrinsic:   (4,4) world -> camera matrix\n",
    "    Returns:     (H, W, 3) world-space points\n",
    "    \"\"\"\n",
    "\n",
    "    R = extrinsic[:3, :3]      # world->cam rotation\n",
    "    t = extrinsic[:3, 3]       # world->cam translation\n",
    "\n",
    "    # Inverse transform:\n",
    "    R_inv = R.T                # cam->world rotation\n",
    "    t_inv = -R_inv @ t         # cam->world translation\n",
    "\n",
    "    # reshape points: (H,W,3) -> (H*W,3)\n",
    "    H, W = cam_point_3d.shape[:2]\n",
    "    pts = cam_point_3d.reshape(-1, 3).T   # (3, HW)\n",
    "\n",
    "    # apply transform\n",
    "    pts_w = R_inv @ pts + t_inv[:, None]  # (3, HW)\n",
    "\n",
    "    # reshape back\n",
    "    return pts_w.T.reshape(H, W, 3)\n",
    "\n",
    "def world_to_camera(world_point_3d, extrinsic):\n",
    "    \"\"\"\n",
    "    world_point_3d: (H, W, 3) world-space 3D points\n",
    "    extrinsic:      (4,4) world -> camera matrix\n",
    "    Returns:        (H, W, 3) camera-space points\n",
    "    \"\"\"\n",
    "\n",
    "    R = extrinsic[:3, :3]      # world->cam rotation\n",
    "    t = extrinsic[:3, 3]       # world->cam translation\n",
    "\n",
    "    # reshape points: (H,W,3) -> (H*W,3)\n",
    "    H, W = world_point_3d.shape[:2]\n",
    "    pts = world_point_3d.reshape(-1, 3).T   # (3, HW)\n",
    "\n",
    "    # apply transform\n",
    "    pts_c = R @ pts + t[:, None]  # (3, HW)\n",
    "\n",
    "    # reshape back\n",
    "    return pts_c.T.reshape(H, W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f40dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_depth_bilinear(point_2d, depth_map):\n",
    "    \"\"\"\n",
    "    point_2d: (N, 2) tensor of 2D pixel coordinates\n",
    "    depth_map: (H, W) tensor of depth values\n",
    "    Returns:\n",
    "        sampled_depths: (N,) tensor of sampled depth values\n",
    "    \"\"\"\n",
    "    H, W = depth_map.shape\n",
    "    x = point_2d[:, 0]\n",
    "    y = point_2d[:, 1]\n",
    "\n",
    "    x0 = torch.floor(x).long().clamp(0, W - 1)\n",
    "    x1 = (x0 + 1).clamp(0, W - 1)\n",
    "    y0 = torch.floor(y).long().clamp(0, H - 1)\n",
    "    y1 = (y0 + 1).clamp(0, H - 1)\n",
    "\n",
    "    Ia = depth_map[y0, x0]\n",
    "    Ib = depth_map[y1, x0]\n",
    "    Ic = depth_map[y0, x1]\n",
    "    Id = depth_map[y1, x1]\n",
    "\n",
    "    wa = (x1.float() - x) * (y1.float() - y)\n",
    "    wb = (x1.float() - x) * (y - y0.float())\n",
    "    wc = (x - x0.float()) * (y1.float() - y)\n",
    "    wd = (x - x0.float()) * (y - y0.float())\n",
    "\n",
    "    sampled_depths = wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
    "\n",
    "    return sampled_depths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc7e68",
   "metadata": {},
   "source": [
    "# Batched ultilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eda14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "# ---- helpers (slightly cleaned) ----\n",
    "def _pixel_grid(H, W, device, dtype):\n",
    "    # Returns pixel coords u (x) and v (y) shaped (H, W)\n",
    "    v, u = torch.meshgrid(\n",
    "        torch.arange(H, device=device, dtype=dtype),\n",
    "        torch.arange(W, device=device, dtype=dtype),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    return u, v  # u->x, v->y\n",
    "\n",
    "def batched_ray_dirs(intrinsics, H, W):\n",
    "    \"\"\"\n",
    "    intrinsics: (B, V, 3, 3)\n",
    "    returns: dirs (B, V, H, W, 3) normalized camera ray directions\n",
    "    \"\"\"\n",
    "    device = intrinsics.device\n",
    "    dtype = intrinsics.dtype\n",
    "    B, V = intrinsics.shape[0], intrinsics.shape[1]\n",
    "\n",
    "    u, v = _pixel_grid(H, W, device, dtype)   # (H, W) u=x, v=y\n",
    "    u = u.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    v = v.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "    fx = intrinsics[..., 0, 0].unsqueeze(-1).unsqueeze(-1)  # (B,V,1,1)\n",
    "    fy = intrinsics[..., 1, 1].unsqueeze(-1).unsqueeze(-1)\n",
    "    cx = intrinsics[..., 0, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "    cy = intrinsics[..., 1, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    dirs_x = (u - cx) / (fx + EPS)   # (B,V,H,W)\n",
    "    dirs_y = (v - cy) / (fy + EPS)\n",
    "    dirs_z = torch.ones_like(dirs_x)\n",
    "\n",
    "    dirs = torch.stack([dirs_x, dirs_y, dirs_z], dim=-1)  # (B,V,H,W,3)\n",
    "    norm = torch.norm(dirs, dim=-1, keepdim=True)\n",
    "    dirs = dirs / (norm + EPS)\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def batched_depth_to_camera(depth, ray_dirs):\n",
    "    # depth: (B,V,H,W), ray_dirs: (B,V,H,W,3)\n",
    "    return depth.unsqueeze(-1) * ray_dirs  # (B,V,H,W,3)\n",
    "\n",
    "\n",
    "def batched_camera_to_world(cam_pts, extrinsics):\n",
    "    \"\"\"\n",
    "    cam_pts: (B, V, H, W, 3)\n",
    "    extrinsics: (B, V, 4, 4)  (world -> camera)\n",
    "    returns: world_pts (B, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    B, V, H, W, _ = cam_pts.shape\n",
    "    R = extrinsics[..., :3, :3]         # (B,V,3,3) world->cam\n",
    "    t = extrinsics[..., :3, 3]          # (B,V,3)\n",
    "\n",
    "    R_inv = R.transpose(-1, -2)         # (B,V,3,3) cam->world\n",
    "    t_inv = -torch.matmul(R_inv, t.unsqueeze(-1)).squeeze(-1)  # (B,V,3)\n",
    "\n",
    "    pts = cam_pts.reshape(B, V, -1, 3)  # (B,V,HW,3)\n",
    "    pts_t = pts.permute(0,1,3,2)        # (B,V,3,HW)\n",
    "    world = torch.matmul(R_inv, pts_t) + t_inv.unsqueeze(-1)    # (B,V,3,HW)\n",
    "    world = world.permute(0,1,3,2).reshape(B, V, H, W, 3)\n",
    "    return world\n",
    "\n",
    "\n",
    "def batched_world_to_camera(world_pts, extrinsics):\n",
    "    \"\"\"\n",
    "    world_pts: (B, V, H, W, 3)\n",
    "    extrinsics: (B, V, 4, 4)  (world -> camera)\n",
    "    returns: cam_pts (B, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    B, V, H, W, _ = world_pts.shape\n",
    "    R = extrinsics[..., :3, :3]    # (B,V,3,3)\n",
    "    t = extrinsics[..., :3, 3]     # (B,V,3)\n",
    "\n",
    "    pts = world_pts.reshape(B, V, -1, 3).permute(0,1,3,2)  # (B,V,3,HW)\n",
    "    cam = torch.matmul(R, pts) + t.unsqueeze(-1)           # (B,V,3,HW)\n",
    "    cam = cam.permute(0,1,3,2).reshape(B, V, H, W, 3)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def batched_project_3d_to_2d(pts_3d, intrinsics):\n",
    "    \"\"\"\n",
    "    pts_3d: (B, V, H, W, 3) in camera coords\n",
    "    intrinsics: (B, V, 3, 3)\n",
    "    returns: uv (B, V, H, W, 2), z (B,V,H,W)\n",
    "    \"\"\"\n",
    "    x = pts_3d[..., 0]\n",
    "    y = pts_3d[..., 1]\n",
    "    z = pts_3d[..., 2].clamp(min=EPS)\n",
    "\n",
    "    z = torch.clamp(z, min=1e-4)\n",
    "\n",
    "    fx = intrinsics[..., 0, 0].unsqueeze(-1).unsqueeze(-1)\n",
    "    fy = intrinsics[..., 1, 1].unsqueeze(-1).unsqueeze(-1)\n",
    "    cx = intrinsics[..., 0, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "    cy = intrinsics[..., 1, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    u = fx * (x / z) + cx\n",
    "    v = fy * (y / z) + cy\n",
    "    uv = torch.stack([u, v], dim=-1)\n",
    "    return uv, z\n",
    "\n",
    "\n",
    "def normalize_uv_for_grid_sample(uv, H, W):\n",
    "    \"\"\"\n",
    "    uv: (..., 2) pixel coords with u in [0..W-1], v in [0..H-1]\n",
    "    returns: grid coords in [-1,1] last-dim order (x,y) for grid_sample\n",
    "    \"\"\"\n",
    "    u = uv[..., 0]\n",
    "    v = uv[..., 1]\n",
    "\n",
    "    nx = (u / (W - 1)) * 2 - 1\n",
    "    ny = (v / (H - 1)) * 2 - 1\n",
    "    return torch.stack([nx, ny], dim=-1)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# UNIT DETECTION & AUTO-SCALING\n",
    "# ================================================================\n",
    "def camera_centers_from_extrinsics(extrinsics):\n",
    "    \"\"\"\n",
    "    extrinsics: (B, V, 4, 4) world->camera\n",
    "    returns: camera centers in world coords (B, V, 3)\n",
    "    \"\"\"\n",
    "    R = extrinsics[..., :3, :3]   # (B,V,3,3)\n",
    "    t = extrinsics[..., :3, 3]    # (B,V,3)\n",
    "    R_inv = R.transpose(-1, -2)   # cam->world\n",
    "    cam_centers = -torch.matmul(R_inv, t.unsqueeze(-1)).squeeze(-1)  # (B,V,3)\n",
    "    return cam_centers\n",
    "\n",
    "\n",
    "def detect_and_fix_depth_unit(depth_batch, extrinsics, threshold_scale=10.0, apply_fix=True):\n",
    "    \"\"\"\n",
    "    Heuristic: if median depth >> median camera-translation magnitude,\n",
    "    likely depth is in mm (or cm). We scale down by 1000 or 100 accordingly.\n",
    "    Returns (depth_batch_scaled, scale_factor, did_scale_flag, med_scale, med_depth_mean, baseline_mean)\n",
    "    \"\"\"\n",
    "    device = depth_batch.device\n",
    "    B, V, H, W = depth_batch.shape\n",
    "\n",
    "    # stats on depth center\n",
    "    depth_center = depth_batch[:, 1]  # (B,H,W)\n",
    "    med_depth = torch.median(depth_center.reshape(B, -1), dim=1).values  # (B,)\n",
    "\n",
    "    # camera center distances (per sample)\n",
    "    centers = camera_centers_from_extrinsics(extrinsics)  # (B,V,3)\n",
    "    # baseline magnitude between src(0) and center(1)\n",
    "    baseline = torch.norm(centers[:, 0] - centers[:, 1], dim=-1)  # (B,)\n",
    "    # avoid zero baseline\n",
    "    baseline = baseline + 1e-6\n",
    "\n",
    "    scale_factors = med_depth / baseline  # if >> threshold -> likely depth in mm\n",
    "    med_scale = float(torch.median(scale_factors).item())\n",
    "\n",
    "    # Decide conversion\n",
    "    scale = 1.0\n",
    "    did_scale = False\n",
    "    if med_scale > threshold_scale:\n",
    "        # typical: depth in mm -> divide by 1000\n",
    "        if med_scale > 1000:\n",
    "            scale = 1.0 / 1000.0\n",
    "        else:\n",
    "            # try 1000 first\n",
    "            scale = 1.0 / 1000.0\n",
    "        if apply_fix:\n",
    "            depth_batch = depth_batch * scale\n",
    "            did_scale = True\n",
    "\n",
    "    return depth_batch, scale, did_scale, med_scale, float(med_depth.mean().item()), float(baseline.mean().item())\n",
    "\n",
    "\n",
    "def reprojection_pair_to_center(depth_batch, intrinsics, extrinsics,\n",
    "                                 center_idx=1, src_idx=0, center_depth_override=None):\n",
    "    \"\"\"\n",
    "    Reproject source view to center view via 3D world points.\n",
    "    \n",
    "    Args:\n",
    "        depth_batch: (B, V, H, W) depth maps\n",
    "        intrinsics: (B, V, 3, 3) camera intrinsics\n",
    "        extrinsics: (B, V, 4, 4) camera extrinsics (world->camera)\n",
    "        center_idx: index of center view\n",
    "        src_idx: index of source view to reproject\n",
    "        center_depth_override: optional (B, H, W) predicted depth for center view\n",
    "    \n",
    "    Returns:\n",
    "        X_src_world: (B, H, W, 3) 3D points from source view\n",
    "        X_center_world: (B, H, W, 3) reprojected 3D points in center view\n",
    "        valid: (B, H, W) valid pixel mask\n",
    "        uv: (B, H, W, 2) projected pixel coordinates\n",
    "    \"\"\"\n",
    "    B, V, H, W = depth_batch.shape\n",
    "    EPS = 1e-4\n",
    "    \n",
    "    # Compute ray directions for all views\n",
    "    ray_dirs = batched_ray_dirs(intrinsics, H, W)\n",
    "    \n",
    "    # Backproject source depth to 3D camera space\n",
    "    depth_src = torch.clamp(depth_batch[:, src_idx], min=EPS, max=1e6).unsqueeze(-1)\n",
    "    cam_pts_src = depth_src * ray_dirs[:, src_idx]\n",
    "    \n",
    "    # Transform source points to world space\n",
    "    X_src_world = batched_camera_to_world(\n",
    "        cam_pts_src.unsqueeze(1), \n",
    "        extrinsics[:, src_idx:src_idx+1]\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Transform world points to center camera space\n",
    "    X_src_in_center = batched_world_to_camera(\n",
    "        X_src_world.unsqueeze(1),\n",
    "        extrinsics[:, center_idx:center_idx+1]\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Clamp depth (z) to positive values (avoid in-place ops)\n",
    "    X_src_in_center = torch.cat([\n",
    "        X_src_in_center[..., :2],\n",
    "        X_src_in_center[..., 2:3].clamp(min=EPS)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    # Project to 2D pixel coordinates in center view\n",
    "    uv, z_proj = batched_project_3d_to_2d(\n",
    "        X_src_in_center.unsqueeze(1),\n",
    "        intrinsics[:, center_idx:center_idx+1]\n",
    "    )\n",
    "    uv = uv.squeeze(1)\n",
    "    z_proj = z_proj.squeeze(1)\n",
    "    \n",
    "    # Clamp UV to image bounds\n",
    "    uv = torch.stack([\n",
    "        uv[..., 0].clamp(0.0, W - 1.0),\n",
    "        uv[..., 1].clamp(0.0, H - 1.0)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    # Sample depth at projected coordinates\n",
    "    grid = normalize_uv_for_grid_sample(uv, H, W)\n",
    "    depth_center = (center_depth_override if center_depth_override is not None \n",
    "                   else depth_batch[:, center_idx])\n",
    "    depth_center = torch.clamp(depth_center, min=EPS, max=1e6).unsqueeze(1)\n",
    "    \n",
    "    sampled_depth = F.grid_sample(\n",
    "        depth_center, grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='zeros', \n",
    "        align_corners=True\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Sample ray directions at projected coordinates\n",
    "    ray_center = ray_dirs[:, center_idx].permute(0, 3, 1, 2)\n",
    "    sampled_rays = F.grid_sample(\n",
    "        ray_center, grid,\n",
    "        mode='bilinear',\n",
    "        padding_mode='zeros',\n",
    "        align_corners=True\n",
    "    ).permute(0, 2, 3, 1)\n",
    "    \n",
    "    # Backproject sampled center depth to world space\n",
    "    cam_pts_center = sampled_depth.unsqueeze(-1) * sampled_rays\n",
    "    X_center_world = batched_camera_to_world(\n",
    "        cam_pts_center.unsqueeze(1),\n",
    "        extrinsics[:, center_idx:center_idx+1]\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Compute valid mask\n",
    "    in_bounds = (uv[..., 0] >= 0) & (uv[..., 0] < W) & \\\n",
    "                (uv[..., 1] >= 0) & (uv[..., 1] < H)\n",
    "    valid = in_bounds & (z_proj > 0) & (sampled_depth > 0)\n",
    "    \n",
    "    return X_src_world, X_center_world, valid, uv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b73ba2",
   "metadata": {},
   "source": [
    "# Loss fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b189b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Loss Functions\n",
    "# ===========================\n",
    "\n",
    "def affine_align_depth(depth_init, gt_depth, mask, eps=1e-6):\n",
    "    \"\"\"Compute affine alignment: s, t such that s*depth_init + t √¢‚Ä∞ÀÜ gt_depth\"\"\"\n",
    "    B = depth_init.shape[0]\n",
    "    s_list, t_list = [], []\n",
    "    depth_aligned = torch.zeros_like(depth_init)\n",
    "    \n",
    "    for b in range(B):\n",
    "        m = mask[b, 0].reshape(-1)\n",
    "        if m.sum() < 10:\n",
    "            s_list.append(1.0)\n",
    "            t_list.append(0.0)\n",
    "            depth_aligned[b] = depth_init[b]\n",
    "            continue\n",
    "        \n",
    "        d = depth_init[b, 0].reshape(-1)[m]\n",
    "        g = gt_depth[b, 0].reshape(-1)[m]\n",
    "        A = torch.stack([d, torch.ones_like(d)], dim=1)\n",
    "        g_col = g.unsqueeze(1)\n",
    "        \n",
    "        try:\n",
    "            x = torch.linalg.lstsq(A, g_col).solution\n",
    "            s, t = float(x[0].item()), float(x[1].item())\n",
    "        except:\n",
    "            s, t = 1.0, 0.0\n",
    "        \n",
    "        s_list.append(s)\n",
    "        t_list.append(t)\n",
    "        depth_aligned[b, 0] = depth_init[b, 0] * s + t\n",
    "    \n",
    "    return depth_aligned, s_list, t_list\n",
    "\n",
    "\n",
    "def edge_aware_smoothness(depth, rgb):\n",
    "    \"\"\"Edge-aware smoothness: smooth depth except at RGB edges\"\"\"\n",
    "    grad_depth_x = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n",
    "    grad_depth_y = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n",
    "    grad_rgb_x = torch.mean(torch.abs(rgb[:, :, :, :-1] - rgb[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    grad_rgb_y = torch.mean(torch.abs(rgb[:, :, :-1, :] - rgb[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    return torch.mean(grad_depth_x * torch.exp(-grad_rgb_x)) + torch.mean(grad_depth_y * torch.exp(-grad_rgb_y))\n",
    "\n",
    "\n",
    "def normals_from_depth(depth, intrinsics):\n",
    "    \"\"\"Compute surface normals from depth\"\"\"\n",
    "    B = depth.shape[0]\n",
    "    intr_center = intrinsics[:, 0]\n",
    "    fx = intr_center[:, 0, 0].view(B, 1, 1, 1)\n",
    "    fy = intr_center[:, 1, 1].view(B, 1, 1, 1)\n",
    "    \n",
    "    dz_dx = (depth[:, :, :, 1:] - depth[:, :, :, :-1]) / (fx + 1e-6)\n",
    "    dz_dy = (depth[:, :, 1:, :] - depth[:, :, :-1, :]) / (fy + 1e-6)\n",
    "    dz_dx = torch.cat([dz_dx, dz_dx[:, :, :, -1:]], dim=3)\n",
    "    dz_dy = torch.cat([dz_dy, dz_dy[:, :, -1:, :]], dim=2)\n",
    "    \n",
    "    normals = torch.stack([-dz_dx, -dz_dy, torch.ones_like(dz_dx)], dim=-1)\n",
    "    norm = torch.norm(normals, dim=-1, keepdim=True) + 1e-6\n",
    "    return normals / norm\n",
    "\n",
    "\n",
    "def normal_smoothness_loss(normals):\n",
    "    \"\"\"Encourage smooth normals\"\"\"\n",
    "    n = normals\n",
    "    return torch.mean(torch.abs(n[:, :, :, :-1, :] - n[:, :, :, 1:, :])) + \\\n",
    "           torch.mean(torch.abs(n[:, :, :-1, :, :] - n[:, :, 1:, :, :]))\n",
    "\n",
    "\n",
    "def masked_l1_huber(pred, gt, mask, eps=1e-6, beta=0.1):\n",
    "    \"\"\"Masked Huber loss\"\"\"\n",
    "    valid = mask.float()\n",
    "    pred_safe = torch.clamp(pred, min=0.0, max=1e4)\n",
    "    gt_safe = torch.clamp(gt, min=0.0, max=1e4)\n",
    "    huber = torch.nn.SmoothL1Loss(reduction='none', beta=beta)\n",
    "    loss = huber(pred_safe, gt_safe)\n",
    "    loss = torch.nan_to_num(loss, nan=0.0, posinf=1e2, neginf=0.0)\n",
    "    loss = torch.clamp(loss, min=0.0, max=1e2)\n",
    "    masked_loss = (loss * valid).sum() / (valid.sum() + eps)\n",
    "    return torch.nan_to_num(masked_loss, nan=0.0, posinf=1e2, neginf=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d0917",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed028659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device, num_steps=None,\n",
    "             lambda_mv=0.001, lambda_gt=1.0):\n",
    "    \"\"\"\n",
    "    Validation function - evaluates model on validation set\n",
    "    Uses cached DepthAnything predictions from dataset\n",
    "    Returns: (avg_total_loss, avg_mv_loss, avg_gt_loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    GT_INVALID_THRESH = 100.0\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    total_loss_sum = 0.0\n",
    "    total_mv_loss_sum = 0.0\n",
    "    total_gt_loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            if num_steps is not None and step >= num_steps:\n",
    "                break\n",
    "            \n",
    "            batch_rgb = batch['rgb'].to(device)\n",
    "            batch_depth = batch['depth'].to(device)\n",
    "            batch_depth_init = batch['depth_init'].to(device)  # Cached DepthAnything\n",
    "            batch_intr = batch['intrinsic'].to(device)\n",
    "            batch_extr = batch['extrinsic'].to(device)\n",
    "            \n",
    "            B, V, _, H, W = batch_rgb.shape\n",
    "            center_idx = V // 2\n",
    "            \n",
    "            # Use cached DepthAnything predictions (already normalized [0,1])\n",
    "            depth_init_all = torch.clamp(batch_depth_init, min=1e-4, max=1.0)\n",
    "            \n",
    "            # Detect & fix depth unit mismatch\n",
    "            depth_init_scaled, _, _, _, _, _ = \\\n",
    "                detect_and_fix_depth_unit(batch_depth, batch_extr, threshold_scale=10.0, apply_fix=True)\n",
    "            \n",
    "            # Prepare center view depth\n",
    "            depth_init_center = depth_init_all[:, center_idx].unsqueeze(1)\n",
    "            gt_center = depth_init_scaled[:, center_idx].unsqueeze(1)\n",
    "            gt_valid = (~torch.isinf(gt_center)) & (gt_center < GT_INVALID_THRESH) & (gt_center > 0)\n",
    "            n_valid_gt = gt_valid.float().sum().item()\n",
    "            \n",
    "            # Affine alignment\n",
    "            try:\n",
    "                depth_init_aligned, s_scales, t_offsets = affine_align_depth(depth_init_center, gt_center, gt_valid)\n",
    "            except:\n",
    "                depth_init_aligned = depth_init_center.clone()\n",
    "                s_scales = [1.0] * B\n",
    "                t_offsets = [0.0] * B\n",
    "            \n",
    "            # Prepare UNet input\n",
    "            ray_dirs_batch = batched_ray_dirs(batch_intr, H, W)\n",
    "            rgb_center = batch_rgb[:, center_idx]\n",
    "            ray_dirs_center = ray_dirs_batch[:, center_idx].permute(0, 3, 1, 2)\n",
    "            model_input = torch.cat([rgb_center, depth_init_aligned, ray_dirs_center], dim=1)\n",
    "            \n",
    "            # Forward pass\n",
    "            depth_delta = model(model_input)\n",
    "            pred_depth_center = depth_init_aligned + depth_delta\n",
    "            pred_depth_center = torch.clamp(pred_depth_center, min=1e-4, max=1e6)\n",
    "            \n",
    "            # Multi-view reprojection loss\n",
    "            mv_loss = 0.0\n",
    "            for src_idx in [0, 2]:\n",
    "                X_src, X_center, valid, _ = reprojection_pair_to_center(\n",
    "                    depth_init_scaled, batch_intr, batch_extr,\n",
    "                    center_idx=center_idx, src_idx=src_idx,\n",
    "                    center_depth_override=pred_depth_center.squeeze(1)\n",
    "                )\n",
    "                error_3d = torch.norm(X_src - X_center, dim=-1)\n",
    "                mv_loss += (error_3d * valid.float()).sum() / (valid.float().sum() + EPS)\n",
    "            mv_loss = mv_loss / 2.0\n",
    "            \n",
    "            # GT supervised loss\n",
    "            L_gt = 0.0\n",
    "            if n_valid_gt > 10:\n",
    "                s_t = torch.tensor(s_scales, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "                t_t = torch.tensor(t_offsets, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "                pred_aligned = pred_depth_center * s_t + t_t\n",
    "                L_gt = masked_l1_huber(pred_aligned, gt_center, gt_valid, beta=0.1)\n",
    "            \n",
    "            # Combined loss\n",
    "            total_loss = lambda_mv * mv_loss + lambda_gt * L_gt\n",
    "            \n",
    "            total_loss_sum += total_loss.item()\n",
    "            total_mv_loss_sum += mv_loss.item()\n",
    "            total_gt_loss_sum += L_gt.item() if isinstance(L_gt, torch.Tensor) else L_gt\n",
    "            num_batches += 1\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    return (total_loss_sum / num_batches, \n",
    "            total_mv_loss_sum / num_batches, \n",
    "            total_gt_loss_sum / num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20674e85",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1917e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Training Loop\n",
    "# ===========================\n",
    "\n",
    "def train_one_epoch(model, train_loader, val_loader, optimizer, device, \n",
    "                    epoch=0, num_epochs=1, use_gt_for_loss=True, \n",
    "                    lambda_mv=0.001, lambda_init=0.1, lambda_edge=0.05, \n",
    "                    lambda_norm=0.1, eval_steps=None):\n",
    "    \"\"\"\n",
    "    Training epoch with multi-view consistency and Huber loss\n",
    "    Uses cached DepthAnything predictions from dataset\n",
    "    \n",
    "    Args:\n",
    "        eval_steps: If None, validate once at end of epoch. \n",
    "                   If int, validate every N training steps.\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    model.train()\n",
    "    GT_INVALID_THRESH = 100.0\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    global_step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", \n",
    "                unit=\"batch\", leave=True)\n",
    "    \n",
    "    for step, batch in enumerate(pbar):\n",
    "        global_step = epoch * len(train_loader) + step + 1\n",
    "        \n",
    "        batch_rgb = batch['rgb'].to(device)\n",
    "        batch_depth = batch['depth'].to(device)\n",
    "        batch_depth = batch_depth / 100.0  # Convert GTAV depth from cm to m\n",
    "        batch_depth_init = batch['depth_init'].to(device)  # Cached DepthAnything\n",
    "        batch_intr = batch['intrinsic'].to(device)\n",
    "        batch_extr = batch['extrinsic'].to(device)\n",
    "        \n",
    "        B, V, _, H, W = batch_rgb.shape\n",
    "        center_idx = V // 2\n",
    "        \n",
    "        # Use cached DepthAnything predictions (already normalized [0,1])\n",
    "        depth_init_all = torch.clamp(batch_depth_init, min=1e-4, max=1.0)\n",
    "        \n",
    "        # Detect & fix depth unit mismatch\n",
    "        depth_init_scaled, _, _, _, _, _ = \\\n",
    "            detect_and_fix_depth_unit(batch_depth, batch_extr, threshold_scale=10.0, apply_fix=True)\n",
    "        \n",
    "        # Prepare center view depth\n",
    "        depth_init_center = depth_init_all[:, center_idx].unsqueeze(1)\n",
    "        gt_center = depth_init_scaled[:, center_idx].unsqueeze(1)\n",
    "        gt_valid = (~torch.isinf(gt_center)) & (gt_center < GT_INVALID_THRESH) & (gt_center > 0)\n",
    "        n_valid_gt = gt_valid.float().sum().item()\n",
    "        \n",
    "        # Affine alignment\n",
    "        try:\n",
    "            depth_init_aligned, s_scales, t_offsets = affine_align_depth(depth_init_center, gt_center, gt_valid)\n",
    "        except:\n",
    "            depth_init_aligned = depth_init_center.clone()\n",
    "            s_scales = [1.0] * B\n",
    "            t_offsets = [0.0] * B\n",
    "        \n",
    "        # Prepare UNet input\n",
    "        ray_dirs_batch = batched_ray_dirs(batch_intr, H, W)\n",
    "        rgb_center = batch_rgb[:, center_idx]\n",
    "        ray_dirs_center = ray_dirs_batch[:, center_idx].permute(0, 3, 1, 2)\n",
    "        model_input = torch.cat([rgb_center, depth_init_aligned, ray_dirs_center], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        depth_delta = model(model_input)\n",
    "        pred_depth_center = depth_init_aligned + depth_delta\n",
    "        pred_depth_center = torch.clamp(pred_depth_center, min=1e-4, max=1e6)\n",
    "        \n",
    "        # Multi-view reprojection loss\n",
    "        mv_loss = 0.0\n",
    "        for src_idx in [0, 2]:\n",
    "            X_src, X_center, valid, _ = reprojection_pair_to_center(\n",
    "                depth_init_scaled, batch_intr, batch_extr,\n",
    "                center_idx=center_idx, src_idx=src_idx,\n",
    "                center_depth_override=pred_depth_center.squeeze(1)\n",
    "            )\n",
    "            error_3d = torch.norm(X_src - X_center, dim=-1)\n",
    "            mv_loss += (error_3d * valid.float()).sum() / (valid.float().sum() + EPS)\n",
    "        mv_loss = mv_loss / 2.0\n",
    "        \n",
    "        # GT supervised loss (Huber with affine alignment)\n",
    "        L_gt = 0.0\n",
    "        if use_gt_for_loss and n_valid_gt > 10:\n",
    "            s_t = torch.tensor(s_scales, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "            t_t = torch.tensor(t_offsets, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "            pred_aligned = pred_depth_center * s_t + t_t\n",
    "            L_gt = masked_l1_huber(pred_aligned, gt_center, gt_valid, beta=0.1)\n",
    "        \n",
    "        # Regularization losses\n",
    "        L_init = edge_aware_smoothness(depth_init_aligned, rgb_center)\n",
    "        L_edge = edge_aware_smoothness(pred_depth_center, rgb_center)\n",
    "        normals = normals_from_depth(pred_depth_center, batch_intr)\n",
    "        L_norm = normal_smoothness_loss(normals)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = lambda_mv*mv_loss + 1.0*L_gt + lambda_init*L_init + lambda_edge*L_edge + lambda_norm*L_norm\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar with loss info\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{total_loss:.4f}',\n",
    "            'MV': f'{mv_loss:.2f}',\n",
    "            'GT': f'{L_gt:.4f}'\n",
    "        })\n",
    "        \n",
    "        # Validation check\n",
    "        should_validate = False\n",
    "        if eval_steps is not None:\n",
    "            # Validate every N steps\n",
    "            should_validate = (global_step % eval_steps == 0)\n",
    "        else:\n",
    "            # Validate at end of epoch\n",
    "            should_validate = (step + 1 == len(train_loader))\n",
    "        \n",
    "        if should_validate:\n",
    "            pbar.write(f\"\\n{'='*60}\")\n",
    "            pbar.write(f\"Validation at step {global_step}\")\n",
    "            pbar.write(f\"{'='*60}\")\n",
    "            \n",
    "            val_total, val_mv, val_gt = validate(\n",
    "                model=model,\n",
    "                dataloader=val_loader,\n",
    "                device=device,\n",
    "                num_steps=len(val_loader),\n",
    "                lambda_mv=0.001,\n",
    "                lambda_gt=1.0\n",
    "            )\n",
    "            \n",
    "            pbar.write(f\"Val Loss - Total: {val_total:.4f} | MV: {val_mv:.4f} | GT: {val_gt:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_total < best_val_loss:\n",
    "                best_val_loss = val_total\n",
    "                checkpoint_path = f\"checkpoints/best_model_ep{epoch+1:03d}_step{global_step:05d}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'step': global_step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_total,\n",
    "                    'val_mv_loss': val_mv,\n",
    "                    'val_gt_loss': val_gt,\n",
    "                }, checkpoint_path)\n",
    "                pbar.write(f\"√¢≈ì‚Äú Saved best model: {checkpoint_path} (Val Loss: {val_total:.4f})\")\n",
    "            \n",
    "            model.train()  # Switch back to training mode\n",
    "    \n",
    "    pbar.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "\n",
      "================================================================================\n",
      "Training: 1 epochs\n",
      "Train batches per epoch: 90\n",
      "Val batches: 10\n",
      "Validation: Once per epoch (after all batches)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 1/1\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/90 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Training Configuration\n",
    "# ===========================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = Unet(in_channel=7, out_channel=1, base=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "eval_steps = None  # Validate every N training steps (None = once per epoch)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Training: {num_epochs} epochs\")\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "if eval_steps is not None:\n",
    "    print(f\"Validation: Every {eval_steps} training steps\")\n",
    "else:\n",
    "    print(f\"Validation: Once per epoch (after all batches)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_best_val = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        num_epochs=num_epochs,\n",
    "        use_gt_for_loss=True,\n",
    "        lambda_mv=0.00001,  # Reduced to prevent MV loss explosion\n",
    "        lambda_init=0.1,\n",
    "        lambda_edge=0.05,\n",
    "        lambda_norm=0.1,\n",
    "        eval_steps=eval_steps\n",
    "    )\n",
    "    \n",
    "    if epoch_best_val < best_val_loss:\n",
    "        best_val_loss = epoch_best_val\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
