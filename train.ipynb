{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84423ed",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74a21a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a43e2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6944dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channel=7, out_channel=1, base=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_channel, base)\n",
    "        self.enc2 = ConvBlock(base, base*2)\n",
    "        self.enc3 = ConvBlock(base*2, base*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base*4, base*2)\n",
    "        self.dec1 = ConvBlock(base*2, base)\n",
    "        self.out = nn.Conv2d(base, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        \n",
    "        d2 = self.up2(e3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.out(d1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58c7b3",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dcbfd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_gtav_exr.py\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import OpenEXR\n",
    "import Imath\n",
    "\n",
    "\n",
    "def load_exr_depth(path):\n",
    "    exr = OpenEXR.InputFile(path)\n",
    "    dw = exr.header()['dataWindow']\n",
    "    width  = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "\n",
    "    channel = 'Z' if 'Z' in exr.header()['channels'] else 'Y'\n",
    "    pt = Imath.PixelType(Imath.PixelType.FLOAT)\n",
    "    depth_str = exr.channel(channel, pt)\n",
    "\n",
    "    depth = np.frombuffer(depth_str, dtype=np.float32)\n",
    "    return depth.reshape((height, width))\n",
    "\n",
    "\n",
    "def load_pose_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    extr = np.array(meta[\"extrinsic\"], dtype=np.float32)\n",
    "    fx, fy = meta[\"f_x\"], meta[\"f_y\"]\n",
    "    cx, cy = meta[\"c_x\"], meta[\"c_y\"]\n",
    "\n",
    "    K = np.array([\n",
    "        [fx, 0,  cx],\n",
    "        [0,  fy, cy],\n",
    "        [0,   0,  1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Fix left-handed coordinate issues\n",
    "    R = extr[:3, :3]\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R[:, 2] *= -1\n",
    "        extr[:3, :3] = R\n",
    "\n",
    "    return extr, K\n",
    "\n",
    "\n",
    "class GTAVEXRDataset(Dataset):\n",
    "    def __init__(self, root, n_views=3, max_stride=3):\n",
    "        assert n_views % 2 == 1, \"n_views must be odd: 3,5,7...\"\n",
    "        self.n_views = n_views\n",
    "        self.max_stride = max_stride\n",
    "        self.root = root\n",
    "\n",
    "        self.scenes = []\n",
    "\n",
    "        scene_ids = sorted(os.listdir(root))\n",
    "        for sid in scene_ids:\n",
    "            spath = os.path.join(root, sid)\n",
    "            if not os.path.isdir(spath):\n",
    "                continue\n",
    "\n",
    "            imgs   = sorted(os.listdir(os.path.join(spath, \"images\")))\n",
    "            depths = sorted(os.listdir(os.path.join(spath, \"depths\")))\n",
    "            poses  = sorted(os.listdir(os.path.join(spath, \"poses\")))\n",
    "\n",
    "            self.scenes.append({\n",
    "                \"img\":   [os.path.join(spath, \"images\", f) for f in imgs],\n",
    "                \"depth\": [os.path.join(spath, \"depths\", f) for f in depths],\n",
    "                \"pose\":  [os.path.join(spath, \"poses\", f) for f in poses],\n",
    "                \"len\": len(imgs),\n",
    "                \"root\": spath\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(s[\"len\"] for s in self.scenes)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Generalized symmetric sampling around center (Option 1)\n",
    "    # ----------------------------------------------------------\n",
    "    def sample_views(self, L):\n",
    "        \"\"\"\n",
    "        Returns sorted list of n_views indices:\n",
    "        symmetric around a random center.\n",
    "        \"\"\"\n",
    "        half = self.n_views // 2\n",
    "        # ensure center can shift both sides\n",
    "        c = random.randint(half, L - half - 1)\n",
    "\n",
    "        views = [c]\n",
    "\n",
    "        for h in range(1, half + 1):\n",
    "            # random stride for left/right\n",
    "            k = random.randint(1, self.max_stride)\n",
    "\n",
    "            left  = max(0,     c - k)\n",
    "            right = min(L - 1, c + k)\n",
    "\n",
    "            views.append(left)\n",
    "            views.append(right)\n",
    "\n",
    "        views = sorted(views)[:self.n_views]   # ensure correct count\n",
    "        return views\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # __getitem__\n",
    "    # ----------------------------------------------------------\n",
    "    def __getitem__(self, _):\n",
    "        scene = random.choice(self.scenes)\n",
    "        L = scene[\"len\"]\n",
    "\n",
    "        # pick generalized multi-view indices\n",
    "        idxs = self.sample_views(L)   # e.g. [c-k2, c-k1, c, c+k1, c+k2] for 5 views\n",
    "\n",
    "        # Load RGB\n",
    "        def load_rgb(path):\n",
    "            img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            return img.astype(np.float32) / 255.0\n",
    "\n",
    "        rgb = np.stack([load_rgb(scene[\"img\"][v]) for v in idxs])   # (V,H,W,3)\n",
    "        depth = np.stack([load_exr_depth(scene[\"depth\"][v]) for v in idxs])   # (V,H,W)\n",
    "\n",
    "        # Load extrinsics/intrinsics\n",
    "        extrinsics = []\n",
    "        intrinsics = []\n",
    "\n",
    "        for v in idxs:\n",
    "            extr, K = load_pose_json(scene[\"pose\"][v])\n",
    "            extrinsics.append(extr)\n",
    "            intrinsics.append(K)\n",
    "\n",
    "        extrinsics = np.stack(extrinsics)   # (V,4,4)\n",
    "        intrinsics = np.stack(intrinsics)   # (V,3,3)\n",
    "\n",
    "        return {\n",
    "            \"rgb\": torch.from_numpy(rgb).permute(0, 3, 1, 2),  # (V,3,H,W)\n",
    "            \"depth\": torch.from_numpy(depth).float(),         # (V,H,W)\n",
    "            \"extrinsic\": torch.from_numpy(extrinsics).float(),# (V,4,4)\n",
    "            \"intrinsic\": torch.from_numpy(intrinsics).float() # (V,3,3)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f766cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB: torch.Size([3, 5, 3, 540, 960])\n",
      "Depth: torch.Size([3, 5, 540, 960])\n",
      "Extrinsic: torch.Size([3, 5, 4, 4])\n",
      "Intrinsic: torch.Size([3, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = GTAVEXRDataset(root=\"dataset/GTAV_540\", max_stride=3, n_views=5)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Test 1 batch\n",
    "test_sample = None\n",
    "for batch in dataloader:\n",
    "    test_sample = batch\n",
    "    print(\"RGB:\", batch[\"rgb\"].shape)           # (B, V, 3, H, W) (batch, view_count, channels, width, height)\n",
    "    print(\"Depth:\", batch[\"depth\"].shape)       # (B, V, H, W)\n",
    "    print(\"Extrinsic:\", batch[\"extrinsic\"].shape) # (B, V, 4, 4)\n",
    "    print(\"Intrinsic:\", batch[\"intrinsic\"].shape) # (B, V, 3, 3)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96794dbb",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "275488c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([540, 960, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ray_dirs(intrinsics, W, H):\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        indexing='xy'\n",
    "    ) \n",
    "\n",
    "    dirs = torch.stack([\n",
    "        (i - intrinsics[0, 2]) / intrinsics[0, 0], # [(X_i) - (fx)]/ (B, cx) \n",
    "        (j - intrinsics[1, 2]) / intrinsics[1, 1],\n",
    "        torch.ones_like(i)\n",
    "    ], -1) \n",
    "\n",
    "    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "    return dirs  # (W,H,[dir_x, dir_y, dir_z])\n",
    "\n",
    "# Test\n",
    "W, H = test_sample['rgb'].shape[-1], test_sample['rgb'].shape[-2]\n",
    "calculate_ray_dirs(test_sample[\"intrinsic\"][0,1], W, H).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1bcd393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([540, 960, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def project_depth_to_camera_3d(depth_map, ray_dirs):\n",
    "    return depth_map[..., None] * ray_dirs # return [X, Y, Z] 3D points in camera space\n",
    "\n",
    "def project_3d_to_camera_2d(points_3d, intrinsic):\n",
    "    \"\"\"\n",
    "    points_3d: (..., 3) tensor of 3D points in camera space\n",
    "    intrinsic: (3, 3) camera intrinsic matrix\n",
    "    Returns:\n",
    "        points_2d: (..., 2) tensor of 2D pixel coordinates\n",
    "    \"\"\"\n",
    "    fx = intrinsic[0, 0]\n",
    "    fy = intrinsic[1, 1]\n",
    "    cx = intrinsic[0, 2]\n",
    "    cy = intrinsic[1, 2]\n",
    "\n",
    "    x = points_3d[..., 0]\n",
    "    y = points_3d[..., 1]\n",
    "    z = points_3d[..., 2].clamp(min=1e-6)  # Prevent division by zero\n",
    "\n",
    "    u = fx * (x / z) + cx\n",
    "    v = fy * (y / z) + cy\n",
    "\n",
    "    return torch.stack([u, v], dim=-1)\n",
    "\n",
    "# Test\n",
    "project_depth_to_camera_3d(\n",
    "    test_sample['depth'][0,1], \n",
    "    calculate_ray_dirs(test_sample[\"intrinsic\"][0,1], W, H)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72b87cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_world(cam_point_3d, extrinsic):\n",
    "    \"\"\"\n",
    "    cam_point_3d: (H, W, 3) camera-space 3D points\n",
    "    extrinsic:   (4,4) world -> camera matrix\n",
    "    Returns:     (H, W, 3) world-space points\n",
    "    \"\"\"\n",
    "\n",
    "    R = extrinsic[:3, :3]      # world->cam rotation\n",
    "    t = extrinsic[:3, 3]       # world->cam translation\n",
    "\n",
    "    # Inverse transform:\n",
    "    R_inv = R.T                # cam->world rotation\n",
    "    t_inv = -R_inv @ t         # cam->world translation\n",
    "\n",
    "    # reshape points: (H,W,3) -> (H*W,3)\n",
    "    H, W = cam_point_3d.shape[:2]\n",
    "    pts = cam_point_3d.reshape(-1, 3).T   # (3, HW)\n",
    "\n",
    "    # apply transform\n",
    "    pts_w = R_inv @ pts + t_inv[:, None]  # (3, HW)\n",
    "\n",
    "    # reshape back\n",
    "    return pts_w.T.reshape(H, W, 3)\n",
    "\n",
    "def world_to_camera(world_point_3d, extrinsic):\n",
    "    \"\"\"\n",
    "    world_point_3d: (H, W, 3) world-space 3D points\n",
    "    extrinsic:      (4,4) world -> camera matrix\n",
    "    Returns:        (H, W, 3) camera-space points\n",
    "    \"\"\"\n",
    "\n",
    "    R = extrinsic[:3, :3]      # world->cam rotation\n",
    "    t = extrinsic[:3, 3]       # world->cam translation\n",
    "\n",
    "    # reshape points: (H,W,3) -> (H*W,3)\n",
    "    H, W = world_point_3d.shape[:2]\n",
    "    pts = world_point_3d.reshape(-1, 3).T   # (3, HW)\n",
    "\n",
    "    # apply transform\n",
    "    pts_c = R @ pts + t[:, None]  # (3, HW)\n",
    "\n",
    "    # reshape back\n",
    "    return pts_c.T.reshape(H, W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5f40dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_depth_bilinear(point_2d, depth_map):\n",
    "    \"\"\"\n",
    "    point_2d: (N, 2) tensor of 2D pixel coordinates\n",
    "    depth_map: (H, W) tensor of depth values\n",
    "    Returns:\n",
    "        sampled_depths: (N,) tensor of sampled depth values\n",
    "    \"\"\"\n",
    "    H, W = depth_map.shape\n",
    "    x = point_2d[:, 0]\n",
    "    y = point_2d[:, 1]\n",
    "\n",
    "    x0 = torch.floor(x).long().clamp(0, W - 1)\n",
    "    x1 = (x0 + 1).clamp(0, W - 1)\n",
    "    y0 = torch.floor(y).long().clamp(0, H - 1)\n",
    "    y1 = (y0 + 1).clamp(0, H - 1)\n",
    "\n",
    "    Ia = depth_map[y0, x0]\n",
    "    Ib = depth_map[y1, x0]\n",
    "    Ic = depth_map[y0, x1]\n",
    "    Id = depth_map[y1, x1]\n",
    "\n",
    "    wa = (x1.float() - x) * (y1.float() - y)\n",
    "    wb = (x1.float() - x) * (y - y0.float())\n",
    "    wc = (x - x0.float()) * (y1.float() - y)\n",
    "    wd = (x - x0.float()) * (y - y0.float())\n",
    "\n",
    "    sampled_depths = wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
    "\n",
    "    return sampled_depths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc7e68",
   "metadata": {},
   "source": [
    "# Batched ultilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "# ---- helpers (slightly cleaned) ----\n",
    "def _pixel_grid(H, W, device, dtype):\n",
    "    # Returns pixel coords u (x) and v (y) shaped (H, W)\n",
    "    v, u = torch.meshgrid(\n",
    "        torch.arange(H, device=device, dtype=dtype),\n",
    "        torch.arange(W, device=device, dtype=dtype),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    return u, v  # u->x, v->y\n",
    "\n",
    "def batched_ray_dirs(intrinsics, H, W):\n",
    "    \"\"\"\n",
    "    intrinsics: (B, V, 3, 3)\n",
    "    returns: dirs (B, V, H, W, 3) normalized camera ray directions\n",
    "    \"\"\"\n",
    "    device = intrinsics.device\n",
    "    dtype = intrinsics.dtype\n",
    "    B, V = intrinsics.shape[0], intrinsics.shape[1]\n",
    "\n",
    "    u, v = _pixel_grid(H, W, device, dtype)   # (H, W) u=x, v=y\n",
    "    u = u.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    v = v.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "    fx = intrinsics[..., 0, 0].unsqueeze(-1).unsqueeze(-1)  # (B,V,1,1)\n",
    "    fy = intrinsics[..., 1, 1].unsqueeze(-1).unsqueeze(-1)\n",
    "    cx = intrinsics[..., 0, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "    cy = intrinsics[..., 1, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    dirs_x = (u - cx) / (fx + EPS)   # (B,V,H,W)\n",
    "    dirs_y = (v - cy) / (fy + EPS)\n",
    "    dirs_z = torch.ones_like(dirs_x)\n",
    "\n",
    "    dirs = torch.stack([dirs_x, dirs_y, dirs_z], dim=-1)  # (B,V,H,W,3)\n",
    "    norm = torch.norm(dirs, dim=-1, keepdim=True)\n",
    "    dirs = dirs / (norm + EPS)\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def batched_depth_to_camera(depth, ray_dirs):\n",
    "    # depth: (B,V,H,W), ray_dirs: (B,V,H,W,3)\n",
    "    return depth.unsqueeze(-1) * ray_dirs  # (B,V,H,W,3)\n",
    "\n",
    "\n",
    "def batched_camera_to_world(cam_pts, extrinsics):\n",
    "    \"\"\"\n",
    "    cam_pts: (B, V, H, W, 3)\n",
    "    extrinsics: (B, V, 4, 4)  (world -> camera)\n",
    "    returns: world_pts (B, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    B, V, H, W, _ = cam_pts.shape\n",
    "    R = extrinsics[..., :3, :3]         # (B,V,3,3) world->cam\n",
    "    t = extrinsics[..., :3, 3]          # (B,V,3)\n",
    "\n",
    "    R_inv = R.transpose(-1, -2)         # (B,V,3,3) cam->world\n",
    "    t_inv = -torch.matmul(R_inv, t.unsqueeze(-1)).squeeze(-1)  # (B,V,3)\n",
    "\n",
    "    pts = cam_pts.reshape(B, V, -1, 3)  # (B,V,HW,3)\n",
    "    pts_t = pts.permute(0,1,3,2)        # (B,V,3,HW)\n",
    "    world = torch.matmul(R_inv, pts_t) + t_inv.unsqueeze(-1)    # (B,V,3,HW)\n",
    "    world = world.permute(0,1,3,2).reshape(B, V, H, W, 3)\n",
    "    return world\n",
    "\n",
    "\n",
    "def batched_world_to_camera(world_pts, extrinsics):\n",
    "    \"\"\"\n",
    "    world_pts: (B, V, H, W, 3)\n",
    "    extrinsics: (B, V, 4, 4)  (world -> camera)\n",
    "    returns: cam_pts (B, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    B, V, H, W, _ = world_pts.shape\n",
    "    R = extrinsics[..., :3, :3]    # (B,V,3,3)\n",
    "    t = extrinsics[..., :3, 3]     # (B,V,3)\n",
    "\n",
    "    pts = world_pts.reshape(B, V, -1, 3).permute(0,1,3,2)  # (B,V,3,HW)\n",
    "    cam = torch.matmul(R, pts) + t.unsqueeze(-1)           # (B,V,3,HW)\n",
    "    cam = cam.permute(0,1,3,2).reshape(B, V, H, W, 3)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def batched_project_3d_to_2d(pts_3d, intrinsics):\n",
    "    \"\"\"\n",
    "    pts_3d: (B, V, H, W, 3) in camera coords\n",
    "    intrinsics: (B, V, 3, 3)\n",
    "    returns: uv (B, V, H, W, 2), z (B,V,H,W)\n",
    "    \"\"\"\n",
    "    x = pts_3d[..., 0]\n",
    "    y = pts_3d[..., 1]\n",
    "    z = pts_3d[..., 2].clamp(min=EPS)\n",
    "\n",
    "    z = torch.clamp(z, min=1e-4)\n",
    "\n",
    "    fx = intrinsics[..., 0, 0].unsqueeze(-1).unsqueeze(-1)\n",
    "    fy = intrinsics[..., 1, 1].unsqueeze(-1).unsqueeze(-1)\n",
    "    cx = intrinsics[..., 0, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "    cy = intrinsics[..., 1, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    u = fx * (x / z) + cx\n",
    "    v = fy * (y / z) + cy\n",
    "    uv = torch.stack([u, v], dim=-1)\n",
    "    return uv, z\n",
    "\n",
    "\n",
    "def normalize_uv_for_grid_sample(uv, H, W):\n",
    "    \"\"\"\n",
    "    uv: (..., 2) pixel coords with u in [0..W-1], v in [0..H-1]\n",
    "    returns: grid coords in [-1,1] last-dim order (x,y) for grid_sample\n",
    "    \"\"\"\n",
    "    u = uv[..., 0]\n",
    "    v = uv[..., 1]\n",
    "\n",
    "    nx = (u / (W - 1)) * 2 - 1\n",
    "    ny = (v / (H - 1)) * 2 - 1\n",
    "    return torch.stack([nx, ny], dim=-1)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# UNIT DETECTION & AUTO-SCALING\n",
    "# ================================================================\n",
    "def camera_centers_from_extrinsics(extrinsics):\n",
    "    \"\"\"\n",
    "    extrinsics: (B, V, 4, 4) world->camera\n",
    "    returns: camera centers in world coords (B, V, 3)\n",
    "    \"\"\"\n",
    "    R = extrinsics[..., :3, :3]   # (B,V,3,3)\n",
    "    t = extrinsics[..., :3, 3]    # (B,V,3)\n",
    "    R_inv = R.transpose(-1, -2)   # cam->world\n",
    "    cam_centers = -torch.matmul(R_inv, t.unsqueeze(-1)).squeeze(-1)  # (B,V,3)\n",
    "    return cam_centers\n",
    "\n",
    "\n",
    "def detect_and_fix_depth_unit(depth_batch, extrinsics, threshold_scale=10.0, apply_fix=True):\n",
    "    \"\"\"\n",
    "    Heuristic: if median depth >> median camera-translation magnitude,\n",
    "    likely depth is in mm (or cm). We scale down by 1000 or 100 accordingly.\n",
    "    Returns (depth_batch_scaled, scale_factor, did_scale_flag, med_scale, med_depth_mean, baseline_mean)\n",
    "    \"\"\"\n",
    "    device = depth_batch.device\n",
    "    B, V, H, W = depth_batch.shape\n",
    "\n",
    "    # stats on depth center\n",
    "    depth_center = depth_batch[:, 1]  # (B,H,W)\n",
    "    med_depth = torch.median(depth_center.reshape(B, -1), dim=1).values  # (B,)\n",
    "\n",
    "    # camera center distances (per sample)\n",
    "    centers = camera_centers_from_extrinsics(extrinsics)  # (B,V,3)\n",
    "    # baseline magnitude between src(0) and center(1)\n",
    "    baseline = torch.norm(centers[:, 0] - centers[:, 1], dim=-1)  # (B,)\n",
    "    # avoid zero baseline\n",
    "    baseline = baseline + 1e-6\n",
    "\n",
    "    scale_factors = med_depth / baseline  # if >> threshold -> likely depth in mm\n",
    "    med_scale = float(torch.median(scale_factors).item())\n",
    "\n",
    "    # Decide conversion\n",
    "    scale = 1.0\n",
    "    did_scale = False\n",
    "    if med_scale > threshold_scale:\n",
    "        # typical: depth in mm -> divide by 1000\n",
    "        if med_scale > 1000:\n",
    "            scale = 1.0 / 1000.0\n",
    "        else:\n",
    "            # try 1000 first\n",
    "            scale = 1.0 / 1000.0\n",
    "        if apply_fix:\n",
    "            depth_batch = depth_batch * scale\n",
    "            did_scale = True\n",
    "\n",
    "    return depth_batch, scale, did_scale, med_scale, float(med_depth.mean().item()), float(baseline.mean().item())\n",
    "\n",
    "\n",
    "def reprojection_pair_to_center(depth_batch, intrinsics, extrinsics,\n",
    "                                 center_idx=1, src_idx=0, center_depth_override=None):\n",
    "    \"\"\"\n",
    "    Reproject source view to center view via 3D world points.\n",
    "    \n",
    "    Args:\n",
    "        depth_batch: (B, V, H, W) depth maps\n",
    "        intrinsics: (B, V, 3, 3) camera intrinsics\n",
    "        extrinsics: (B, V, 4, 4) camera extrinsics (world->camera)\n",
    "        center_idx: index of center view\n",
    "        src_idx: index of source view to reproject\n",
    "        center_depth_override: optional (B, H, W) predicted depth for center view\n",
    "    \n",
    "    Returns:\n",
    "        X_src_world: (B, H, W, 3) 3D points from source view\n",
    "        X_center_world: (B, H, W, 3) reprojected 3D points in center view\n",
    "        valid: (B, H, W) valid pixel mask\n",
    "        uv: (B, H, W, 2) projected pixel coordinates\n",
    "    \"\"\"\n",
    "    B, V, H, W = depth_batch.shape\n",
    "    EPS = 1e-4\n",
    "    \n",
    "    # Compute ray directions for all views\n",
    "    ray_dirs = batched_ray_dirs(intrinsics, H, W)\n",
    "    \n",
    "    # Backproject source depth to 3D camera space\n",
    "    depth_src = torch.clamp(depth_batch[:, src_idx], min=EPS, max=1e6).unsqueeze(-1)\n",
    "    cam_pts_src = depth_src * ray_dirs[:, src_idx]\n",
    "    \n",
    "    # Transform source points to world space\n",
    "    X_src_world = batched_camera_to_world(\n",
    "        cam_pts_src.unsqueeze(1), \n",
    "        extrinsics[:, src_idx:src_idx+1]\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Transform world points to center camera space\n",
    "    X_src_in_center = batched_world_to_camera(\n",
    "        X_src_world.unsqueeze(1),\n",
    "        extrinsics[:, center_idx:center_idx+1]\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Clamp depth (z) to positive values (avoid in-place ops)\n",
    "    X_src_in_center = torch.cat([\n",
    "        X_src_in_center[..., :2],\n",
    "        X_src_in_center[..., 2:3].clamp(min=EPS)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    # Project to 2D pixel coordinates in center view\n",
    "    uv, z_proj = batched_project_3d_to_2d(\n",
    "        X_src_in_center.unsqueeze(1),\n",
    "        intrinsics[:, center_idx:center_idx+1]\n",
    "    )\n",
    "    uv = uv.squeeze(1)\n",
    "    z_proj = z_proj.squeeze(1)\n",
    "    \n",
    "    # Clamp UV to image bounds\n",
    "    uv = torch.stack([\n",
    "        uv[..., 0].clamp(0.0, W - 1.0),\n",
    "        uv[..., 1].clamp(0.0, H - 1.0)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    # Sample depth at projected coordinates\n",
    "    grid = normalize_uv_for_grid_sample(uv, H, W)\n",
    "    depth_center = (center_depth_override if center_depth_override is not None \n",
    "                   else depth_batch[:, center_idx])\n",
    "    depth_center = torch.clamp(depth_center, min=EPS, max=1e6).unsqueeze(1)\n",
    "    \n",
    "    sampled_depth = F.grid_sample(\n",
    "        depth_center, grid, \n",
    "        mode='bilinear', \n",
    "        padding_mode='zeros', \n",
    "        align_corners=True\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Sample ray directions at projected coordinates\n",
    "    ray_center = ray_dirs[:, center_idx].permute(0, 3, 1, 2)\n",
    "    sampled_rays = F.grid_sample(\n",
    "        ray_center, grid,\n",
    "        mode='bilinear',\n",
    "        padding_mode='zeros',\n",
    "        align_corners=True\n",
    "    ).permute(0, 2, 3, 1)\n",
    "    \n",
    "    # Backproject sampled center depth to world space\n",
    "    cam_pts_center = sampled_depth.unsqueeze(-1) * sampled_rays\n",
    "    X_center_world = batched_camera_to_world(\n",
    "        cam_pts_center.unsqueeze(1),\n",
    "        extrinsics[:, center_idx:center_idx+1]\n",
    "    ).squeeze(1)\n",
    "    \n",
    "    # Compute valid mask\n",
    "    in_bounds = (uv[..., 0] >= 0) & (uv[..., 0] < W) & \\\n",
    "                (uv[..., 1] >= 0) & (uv[..., 1] < H)\n",
    "    valid = in_bounds & (z_proj > 0) & (sampled_depth > 0)\n",
    "    \n",
    "    return X_src_world, X_center_world, valid, uv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6153eb",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Wrapper: DepthAnything forward on RGB batch\n",
    "# -------------------------\n",
    "def depth_anything_forward(rgb_batch, depth_anything_model):\n",
    "    \"\"\"\n",
    "    rgb_batch: (B, V, 3, H, W) in [0, 1] float32\n",
    "    Returns: depth_init_all (B, V, H, W) normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    device = rgb_batch.device\n",
    "    B, V, C, H, W = rgb_batch.shape\n",
    "    \n",
    "    depth_init_all = []\n",
    "    \n",
    "    for b in range(B):\n",
    "        depth_batch_b = []\n",
    "        for v in range(V):\n",
    "            # Extract single RGB frame: (3, H, W) -> PIL Image\n",
    "            rgb_frame = rgb_batch[b, v].permute(1, 2, 0)  # (H, W, 3)\n",
    "            rgb_frame_np = (rgb_frame * 255).clamp(0, 255).byte().cpu().numpy()\n",
    "            \n",
    "            # Convert numpy to PIL Image\n",
    "            pil_image = Image.fromarray(rgb_frame_np.astype(np.uint8), mode='RGB')\n",
    "            \n",
    "            # Run Depth Anything inference\n",
    "            with torch.no_grad():\n",
    "                result = depth_anything_model(pil_image)\n",
    "            \n",
    "            # result is a dict with 'depth' key (PIL Image)\n",
    "            depth_pred = result['depth']\n",
    "            \n",
    "            # Convert PIL Image to numpy\n",
    "            if isinstance(depth_pred, Image.Image):\n",
    "                depth_pred = np.array(depth_pred)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            depth_pred = depth_pred.astype(np.float32)\n",
    "            depth_min, depth_max = depth_pred.min(), depth_pred.max()\n",
    "            if depth_max > depth_min:\n",
    "                depth_pred = (depth_pred - depth_min) / (depth_max - depth_min)\n",
    "            else:\n",
    "                depth_pred = np.ones_like(depth_pred) * 0.5\n",
    "            \n",
    "            # Convert back to tensor\n",
    "            depth_pred = torch.from_numpy(depth_pred).float().to(device)  # (H, W)\n",
    "            depth_batch_b.append(depth_pred)\n",
    "        \n",
    "        depth_init_all.append(torch.stack(depth_batch_b, dim=0))  # (V, H, W)\n",
    "    \n",
    "    depth_init_all = torch.stack(depth_init_all, dim=0)  # (B, V, H, W)\n",
    "    return depth_init_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1917e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Loss Functions\n",
    "# ===========================\n",
    "\n",
    "def affine_align_depth(depth_init, gt_depth, mask, eps=1e-6):\n",
    "    \"\"\"Compute affine alignment: s, t such that s*depth_init + t ≈ gt_depth\"\"\"\n",
    "    B = depth_init.shape[0]\n",
    "    s_list, t_list = [], []\n",
    "    depth_aligned = torch.zeros_like(depth_init)\n",
    "    \n",
    "    for b in range(B):\n",
    "        m = mask[b, 0].reshape(-1)\n",
    "        if m.sum() < 10:\n",
    "            s_list.append(1.0)\n",
    "            t_list.append(0.0)\n",
    "            depth_aligned[b] = depth_init[b]\n",
    "            continue\n",
    "        \n",
    "        d = depth_init[b, 0].reshape(-1)[m]\n",
    "        g = gt_depth[b, 0].reshape(-1)[m]\n",
    "        A = torch.stack([d, torch.ones_like(d)], dim=1)\n",
    "        g_col = g.unsqueeze(1)\n",
    "        \n",
    "        try:\n",
    "            x = torch.linalg.lstsq(A, g_col).solution\n",
    "            s, t = float(x[0].item()), float(x[1].item())\n",
    "        except:\n",
    "            s, t = 1.0, 0.0\n",
    "        \n",
    "        s_list.append(s)\n",
    "        t_list.append(t)\n",
    "        depth_aligned[b, 0] = depth_init[b, 0] * s + t\n",
    "    \n",
    "    return depth_aligned, s_list, t_list\n",
    "\n",
    "\n",
    "def edge_aware_smoothness(depth, rgb):\n",
    "    \"\"\"Edge-aware smoothness: smooth depth except at RGB edges\"\"\"\n",
    "    grad_depth_x = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n",
    "    grad_depth_y = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n",
    "    grad_rgb_x = torch.mean(torch.abs(rgb[:, :, :, :-1] - rgb[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    grad_rgb_y = torch.mean(torch.abs(rgb[:, :, :-1, :] - rgb[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    return torch.mean(grad_depth_x * torch.exp(-grad_rgb_x)) + torch.mean(grad_depth_y * torch.exp(-grad_rgb_y))\n",
    "\n",
    "\n",
    "def normals_from_depth(depth, intrinsics):\n",
    "    \"\"\"Compute surface normals from depth\"\"\"\n",
    "    B = depth.shape[0]\n",
    "    intr_center = intrinsics[:, 0]\n",
    "    fx = intr_center[:, 0, 0].view(B, 1, 1, 1)\n",
    "    fy = intr_center[:, 1, 1].view(B, 1, 1, 1)\n",
    "    \n",
    "    dz_dx = (depth[:, :, :, 1:] - depth[:, :, :, :-1]) / (fx + 1e-6)\n",
    "    dz_dy = (depth[:, :, 1:, :] - depth[:, :, :-1, :]) / (fy + 1e-6)\n",
    "    dz_dx = torch.cat([dz_dx, dz_dx[:, :, :, -1:]], dim=3)\n",
    "    dz_dy = torch.cat([dz_dy, dz_dy[:, :, -1:, :]], dim=2)\n",
    "    \n",
    "    normals = torch.stack([-dz_dx, -dz_dy, torch.ones_like(dz_dx)], dim=-1)\n",
    "    norm = torch.norm(normals, dim=-1, keepdim=True) + 1e-6\n",
    "    return normals / norm\n",
    "\n",
    "\n",
    "def normal_smoothness_loss(normals):\n",
    "    \"\"\"Encourage smooth normals\"\"\"\n",
    "    n = normals\n",
    "    return torch.mean(torch.abs(n[:, :, :, :-1, :] - n[:, :, :, 1:, :])) + \\\n",
    "           torch.mean(torch.abs(n[:, :, :-1, :, :] - n[:, :, 1:, :, :]))\n",
    "\n",
    "\n",
    "def masked_l1_huber(pred, gt, mask, eps=1e-6, beta=0.1):\n",
    "    \"\"\"Masked Huber loss\"\"\"\n",
    "    valid = mask.float()\n",
    "    pred_safe = torch.clamp(pred, min=0.0, max=1e4)\n",
    "    gt_safe = torch.clamp(gt, min=0.0, max=1e4)\n",
    "    huber = torch.nn.SmoothL1Loss(reduction='none', beta=beta)\n",
    "    loss = huber(pred_safe, gt_safe)\n",
    "    loss = torch.nan_to_num(loss, nan=0.0, posinf=1e2, neginf=0.0)\n",
    "    loss = torch.clamp(loss, min=0.0, max=1e2)\n",
    "    masked_loss = (loss * valid).sum() / (valid.sum() + eps)\n",
    "    return torch.nan_to_num(masked_loss, nan=0.0, posinf=1e2, neginf=0.0)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Training Loop\n",
    "# ===========================\n",
    "\n",
    "def train_one_epoch(model, depth_anything_model, dataloader, optimizer, device, \n",
    "                    epoch=0, num_epochs=1, use_gt_for_loss=True, \n",
    "                    lambda_mv=0.001, lambda_init=0.1, lambda_edge=0.05, \n",
    "                    lambda_norm=0.1, num_steps=1):\n",
    "    \"\"\"\n",
    "    Training epoch with multi-view consistency and Huber loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    GT_INVALID_THRESH = 100.0\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "        \n",
    "        batch_rgb = batch['rgb'].to(device)\n",
    "        batch_depth = batch['depth'].to(device)\n",
    "        batch_intr = batch['intrinsic'].to(device)\n",
    "        batch_extr = batch['extrinsic'].to(device)\n",
    "        \n",
    "        B, V, _, H, W = batch_rgb.shape\n",
    "        center_idx = V // 2\n",
    "        \n",
    "        print(f\"\\n[Epoch {epoch+1}/{num_epochs}] Step {step+1}/{num_steps}\")\n",
    "        \n",
    "        # DepthAnything inference\n",
    "        depth_init_all = depth_anything_forward(batch_rgb, depth_anything_model)\n",
    "        depth_init_all = torch.clamp(depth_init_all, min=1e-4, max=1.0)\n",
    "        \n",
    "        # Detect & fix depth unit mismatch\n",
    "        depth_init_scaled, _, _, _, _, _ = \\\n",
    "            detect_and_fix_depth_unit(batch_depth, batch_extr, threshold_scale=10.0, apply_fix=True)\n",
    "        \n",
    "        # Prepare center view depth\n",
    "        depth_init_center = depth_init_all[:, center_idx].unsqueeze(1)\n",
    "        gt_center = depth_init_scaled[:, center_idx].unsqueeze(1)\n",
    "        gt_valid = (~torch.isinf(gt_center)) & (gt_center < GT_INVALID_THRESH) & (gt_center > 0)\n",
    "        n_valid_gt = gt_valid.float().sum().item()\n",
    "        \n",
    "        # Affine alignment\n",
    "        try:\n",
    "            depth_init_aligned, s_scales, t_offsets = affine_align_depth(depth_init_center, gt_center, gt_valid)\n",
    "        except:\n",
    "            depth_init_aligned = depth_init_center.clone()\n",
    "            s_scales = [1.0] * B\n",
    "            t_offsets = [0.0] * B\n",
    "        \n",
    "        # Prepare UNet input\n",
    "        ray_dirs_batch = batched_ray_dirs(batch_intr, H, W)\n",
    "        rgb_center = batch_rgb[:, center_idx]\n",
    "        ray_dirs_center = ray_dirs_batch[:, center_idx].permute(0, 3, 1, 2)\n",
    "        model_input = torch.cat([rgb_center, depth_init_aligned, ray_dirs_center], dim=1)\n",
    "        \n",
    "        # Forward pass\n",
    "        depth_delta = model(model_input)\n",
    "        pred_depth_center = depth_init_aligned + depth_delta\n",
    "        pred_depth_center = torch.clamp(pred_depth_center, min=1e-4, max=1e6)\n",
    "        \n",
    "        # Multi-view reprojection loss\n",
    "        mv_loss = 0.0\n",
    "        for src_idx in [0, 2]:\n",
    "            X_src, X_center, valid, _ = reprojection_pair_to_center(\n",
    "                depth_init_scaled, batch_intr, batch_extr,\n",
    "                center_idx=center_idx, src_idx=src_idx,\n",
    "                center_depth_override=pred_depth_center.squeeze(1),\n",
    "                debug=False\n",
    "            )\n",
    "            error_3d = torch.norm(X_src - X_center, dim=-1)\n",
    "            mv_loss += (error_3d * valid.float()).sum() / (valid.float().sum() + EPS)\n",
    "        mv_loss = mv_loss / 2.0\n",
    "        \n",
    "        # GT supervised loss (Huber with affine alignment)\n",
    "        L_gt = 0.0\n",
    "        if use_gt_for_loss and n_valid_gt > 10:\n",
    "            s_t = torch.tensor(s_scales, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "            t_t = torch.tensor(t_offsets, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "            pred_aligned = pred_depth_center * s_t + t_t\n",
    "            L_gt = masked_l1_huber(pred_aligned, gt_center, gt_valid, beta=0.1)\n",
    "        \n",
    "        # Regularization losses\n",
    "        L_init = edge_aware_smoothness(depth_init_aligned, rgb_center)\n",
    "        L_edge = edge_aware_smoothness(pred_depth_center, rgb_center)\n",
    "        normals = normals_from_depth(pred_depth_center, batch_intr)\n",
    "        L_norm = normal_smoothness_loss(normals)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = lambda_mv*mv_loss + 1.0*L_gt + lambda_init*L_init + lambda_edge*L_edge + lambda_norm*L_norm\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log\n",
    "        print(f\"  Total: {total_loss:.4f} | MV: {lambda_mv*mv_loss:.4f} | GT: {L_gt:.4f} | \"\n",
    "              f\"Init: {L_init:.4f} | Edge: {L_edge:.4f} | Norm: {L_norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "157c106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Using] Existing Depth Anything pipeline\n",
      "[Device] cuda\n",
      "\n",
      "================================================================================\n",
      "Training: 1 epochs × 1 steps\n",
      "================================================================================\n",
      "\n",
      "\n",
      "[Epoch 1/1] Step 1/1\n",
      "\n",
      "[Epoch 1/1] Step 1/1\n",
      "  Total: 118.6900 | MV: 118.1199 | GT: 0.5697 | Init: 0.0030 | Edge: 0.0032 | Norm: 0.0000\n",
      "  Total: 118.6900 | MV: 118.1199 | GT: 0.5697 | Init: 0.0030 | Edge: 0.0032 | Norm: 0.0000\n",
      "\n",
      "================================================================================\n",
      "Training completed!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Load Depth Anything Model\n",
    "# ===========================\n",
    "if 'depth_anything' not in dir():\n",
    "    from transformers import pipeline\n",
    "    print(\"[Loading] Depth Anything Small...\")\n",
    "    depth_anything = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-small-hf\")\n",
    "    print(\"[Loaded] Depth Anything Small\")\n",
    "else:\n",
    "    print(\"[Using] Existing Depth Anything pipeline\")\n",
    "\n",
    "# ===========================\n",
    "# Training Configuration\n",
    "# ===========================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"[Device] {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = Unet(in_channel=7, out_channel=1, base=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "steps_per_epoch = 1\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Training: {num_epochs} epochs × {steps_per_epoch} steps\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(\n",
    "        model=model,\n",
    "        depth_anything_model=depth_anything,\n",
    "        dataloader=dataloader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        num_epochs=num_epochs,\n",
    "        use_gt_for_loss=True,\n",
    "        lambda_mv=0.001,  # Reduce MV loss weight to balance with GT\n",
    "        lambda_init=0.1,\n",
    "        lambda_edge=0.05,\n",
    "        lambda_norm=0.1,\n",
    "        num_steps=steps_per_epoch\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23047b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
