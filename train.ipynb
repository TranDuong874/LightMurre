{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84423ed",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74a21a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a43e2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6944dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channel=7, out_channel=1, base=32):\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock(in_channel, base)\n",
    "        self.enc2 = ConvBlock(base, base*2)\n",
    "        self.enc3 = ConvBlock(base*2, base*4)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n",
    "        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(base*4, base*2)\n",
    "        self.dec1 = ConvBlock(base*2, base)\n",
    "        self.out = nn.Conv2d(base, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        \n",
    "        d2 = self.up2(e3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        out = self.out(d1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58c7b3",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dcbfd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_gtav_exr.py\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import OpenEXR\n",
    "import Imath\n",
    "\n",
    "\n",
    "def load_exr_depth(path):\n",
    "    exr = OpenEXR.InputFile(path)\n",
    "    dw = exr.header()['dataWindow']\n",
    "    width  = dw.max.x - dw.min.x + 1\n",
    "    height = dw.max.y - dw.min.y + 1\n",
    "\n",
    "    channel = 'Z' if 'Z' in exr.header()['channels'] else 'Y'\n",
    "    pt = Imath.PixelType(Imath.PixelType.FLOAT)\n",
    "    depth_str = exr.channel(channel, pt)\n",
    "\n",
    "    depth = np.frombuffer(depth_str, dtype=np.float32)\n",
    "    return depth.reshape((height, width))\n",
    "\n",
    "\n",
    "def load_pose_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    extr = np.array(meta[\"extrinsic\"], dtype=np.float32)\n",
    "    fx, fy = meta[\"f_x\"], meta[\"f_y\"]\n",
    "    cx, cy = meta[\"c_x\"], meta[\"c_y\"]\n",
    "\n",
    "    K = np.array([\n",
    "        [fx, 0,  cx],\n",
    "        [0,  fy, cy],\n",
    "        [0,   0,  1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Fix left-handed coordinate issues\n",
    "    R = extr[:3, :3]\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R[:, 2] *= -1\n",
    "        extr[:3, :3] = R\n",
    "\n",
    "    return extr, K\n",
    "\n",
    "\n",
    "class GTAVEXRDataset(Dataset):\n",
    "    def __init__(self, root, n_views=3, max_stride=3):\n",
    "        assert n_views % 2 == 1, \"n_views must be odd: 3,5,7...\"\n",
    "        self.n_views = n_views\n",
    "        self.max_stride = max_stride\n",
    "        self.root = root\n",
    "\n",
    "        self.scenes = []\n",
    "\n",
    "        scene_ids = sorted(os.listdir(root))\n",
    "        for sid in scene_ids:\n",
    "            spath = os.path.join(root, sid)\n",
    "            if not os.path.isdir(spath):\n",
    "                continue\n",
    "\n",
    "            imgs   = sorted(os.listdir(os.path.join(spath, \"images\")))\n",
    "            depths = sorted(os.listdir(os.path.join(spath, \"depths\")))\n",
    "            poses  = sorted(os.listdir(os.path.join(spath, \"poses\")))\n",
    "\n",
    "            self.scenes.append({\n",
    "                \"img\":   [os.path.join(spath, \"images\", f) for f in imgs],\n",
    "                \"depth\": [os.path.join(spath, \"depths\", f) for f in depths],\n",
    "                \"pose\":  [os.path.join(spath, \"poses\", f) for f in poses],\n",
    "                \"len\": len(imgs),\n",
    "                \"root\": spath\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(s[\"len\"] for s in self.scenes)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Generalized symmetric sampling around center (Option 1)\n",
    "    # ----------------------------------------------------------\n",
    "    def sample_views(self, L):\n",
    "        \"\"\"\n",
    "        Returns sorted list of n_views indices:\n",
    "        symmetric around a random center.\n",
    "        \"\"\"\n",
    "        half = self.n_views // 2\n",
    "        # ensure center can shift both sides\n",
    "        c = random.randint(half, L - half - 1)\n",
    "\n",
    "        views = [c]\n",
    "\n",
    "        for h in range(1, half + 1):\n",
    "            # random stride for left/right\n",
    "            k = random.randint(1, self.max_stride)\n",
    "\n",
    "            left  = max(0,     c - k)\n",
    "            right = min(L - 1, c + k)\n",
    "\n",
    "            views.append(left)\n",
    "            views.append(right)\n",
    "\n",
    "        views = sorted(views)[:self.n_views]   # ensure correct count\n",
    "        return views\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # __getitem__\n",
    "    # ----------------------------------------------------------\n",
    "    def __getitem__(self, _):\n",
    "        scene = random.choice(self.scenes)\n",
    "        L = scene[\"len\"]\n",
    "\n",
    "        # pick generalized multi-view indices\n",
    "        idxs = self.sample_views(L)   # e.g. [c-k2, c-k1, c, c+k1, c+k2] for 5 views\n",
    "\n",
    "        # Load RGB\n",
    "        def load_rgb(path):\n",
    "            img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "            return img.astype(np.float32) / 255.0\n",
    "\n",
    "        rgb = np.stack([load_rgb(scene[\"img\"][v]) for v in idxs])   # (V,H,W,3)\n",
    "        depth = np.stack([load_exr_depth(scene[\"depth\"][v]) for v in idxs])   # (V,H,W)\n",
    "\n",
    "        # Load extrinsics/intrinsics\n",
    "        extrinsics = []\n",
    "        intrinsics = []\n",
    "\n",
    "        for v in idxs:\n",
    "            extr, K = load_pose_json(scene[\"pose\"][v])\n",
    "            extrinsics.append(extr)\n",
    "            intrinsics.append(K)\n",
    "\n",
    "        extrinsics = np.stack(extrinsics)   # (V,4,4)\n",
    "        intrinsics = np.stack(intrinsics)   # (V,3,3)\n",
    "\n",
    "        return {\n",
    "            \"rgb\": torch.from_numpy(rgb).permute(0, 3, 1, 2),  # (V,3,H,W)\n",
    "            \"depth\": torch.from_numpy(depth).float(),         # (V,H,W)\n",
    "            \"extrinsic\": torch.from_numpy(extrinsics).float(),# (V,4,4)\n",
    "            \"intrinsic\": torch.from_numpy(intrinsics).float() # (V,3,3)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f766cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB: torch.Size([3, 5, 3, 540, 960])\n",
      "Depth: torch.Size([3, 5, 540, 960])\n",
      "Extrinsic: torch.Size([3, 5, 4, 4])\n",
      "Intrinsic: torch.Size([3, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = GTAVEXRDataset(root=\"dataset/GTAV_540\", max_stride=3, n_views=5)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Test 1 batch\n",
    "test_sample = None\n",
    "for batch in dataloader:\n",
    "    test_sample = batch\n",
    "    print(\"RGB:\", batch[\"rgb\"].shape)           # (B, V, 3, H, W) (batch, view_count, channels, width, height)\n",
    "    print(\"Depth:\", batch[\"depth\"].shape)       # (B, V, H, W)\n",
    "    print(\"Extrinsic:\", batch[\"extrinsic\"].shape) # (B, V, 4, 4)\n",
    "    print(\"Intrinsic:\", batch[\"intrinsic\"].shape) # (B, V, 3, 3)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96794dbb",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "275488c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([540, 960, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ray_dirs(intrinsics, W, H):\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(W, dtype=torch.float32),\n",
    "        torch.arange(H, dtype=torch.float32),\n",
    "        indexing='xy'\n",
    "    ) \n",
    "\n",
    "    dirs = torch.stack([\n",
    "        (i - intrinsics[0, 2]) / intrinsics[0, 0], # [(X_i) - (fx)]/ (B, cx) \n",
    "        (j - intrinsics[1, 2]) / intrinsics[1, 1],\n",
    "        torch.ones_like(i)\n",
    "    ], -1) \n",
    "\n",
    "    dirs = dirs / torch.norm(dirs, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "    return dirs  # (W,H,[dir_x, dir_y, dir_z])\n",
    "\n",
    "# Test\n",
    "W, H = test_sample['rgb'].shape[-1], test_sample['rgb'].shape[-2]\n",
    "calculate_ray_dirs(test_sample[\"intrinsic\"][0,1], W, H).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1bcd393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([540, 960, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def project_depth_to_camera_3d(depth_map, ray_dirs):\n",
    "    return depth_map[..., None] * ray_dirs # return [X, Y, Z] 3D points in camera space\n",
    "\n",
    "def project_3d_to_camera_2d(points_3d, intrinsic):\n",
    "    \"\"\"\n",
    "    points_3d: (..., 3) tensor of 3D points in camera space\n",
    "    intrinsic: (3, 3) camera intrinsic matrix\n",
    "    Returns:\n",
    "        points_2d: (..., 2) tensor of 2D pixel coordinates\n",
    "    \"\"\"\n",
    "    fx = intrinsic[0, 0]\n",
    "    fy = intrinsic[1, 1]\n",
    "    cx = intrinsic[0, 2]\n",
    "    cy = intrinsic[1, 2]\n",
    "\n",
    "    x = points_3d[..., 0]\n",
    "    y = points_3d[..., 1]\n",
    "    z = points_3d[..., 2].clamp(min=1e-6)  # Prevent division by zero\n",
    "\n",
    "    u = fx * (x / z) + cx\n",
    "    v = fy * (y / z) + cy\n",
    "\n",
    "    return torch.stack([u, v], dim=-1)\n",
    "\n",
    "# Test\n",
    "project_depth_to_camera_3d(\n",
    "    test_sample['depth'][0,1], \n",
    "    calculate_ray_dirs(test_sample[\"intrinsic\"][0,1], W, H)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "72b87cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_to_world(cam_point_3d, extrinsic):\n",
    "    \"\"\"\n",
    "    cam_point_3d: (H, W, 3) camera-space 3D points\n",
    "    extrinsic:   (4,4) world -> camera matrix\n",
    "    Returns:     (H, W, 3) world-space points\n",
    "    \"\"\"\n",
    "\n",
    "    R = extrinsic[:3, :3]      # world->cam rotation\n",
    "    t = extrinsic[:3, 3]       # world->cam translation\n",
    "\n",
    "    # Inverse transform:\n",
    "    R_inv = R.T                # cam->world rotation\n",
    "    t_inv = -R_inv @ t         # cam->world translation\n",
    "\n",
    "    # reshape points: (H,W,3) -> (H*W,3)\n",
    "    H, W = cam_point_3d.shape[:2]\n",
    "    pts = cam_point_3d.reshape(-1, 3).T   # (3, HW)\n",
    "\n",
    "    # apply transform\n",
    "    pts_w = R_inv @ pts + t_inv[:, None]  # (3, HW)\n",
    "\n",
    "    # reshape back\n",
    "    return pts_w.T.reshape(H, W, 3)\n",
    "\n",
    "def world_to_camera(world_point_3d, extrinsic):\n",
    "    \"\"\"\n",
    "    world_point_3d: (H, W, 3) world-space 3D points\n",
    "    extrinsic:      (4,4) world -> camera matrix\n",
    "    Returns:        (H, W, 3) camera-space points\n",
    "    \"\"\"\n",
    "\n",
    "    R = extrinsic[:3, :3]      # world->cam rotation\n",
    "    t = extrinsic[:3, 3]       # world->cam translation\n",
    "\n",
    "    # reshape points: (H,W,3) -> (H*W,3)\n",
    "    H, W = world_point_3d.shape[:2]\n",
    "    pts = world_point_3d.reshape(-1, 3).T   # (3, HW)\n",
    "\n",
    "    # apply transform\n",
    "    pts_c = R @ pts + t[:, None]  # (3, HW)\n",
    "\n",
    "    # reshape back\n",
    "    return pts_c.T.reshape(H, W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5f40dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_depth_bilinear(point_2d, depth_map):\n",
    "    \"\"\"\n",
    "    point_2d: (N, 2) tensor of 2D pixel coordinates\n",
    "    depth_map: (H, W) tensor of depth values\n",
    "    Returns:\n",
    "        sampled_depths: (N,) tensor of sampled depth values\n",
    "    \"\"\"\n",
    "    H, W = depth_map.shape\n",
    "    x = point_2d[:, 0]\n",
    "    y = point_2d[:, 1]\n",
    "\n",
    "    x0 = torch.floor(x).long().clamp(0, W - 1)\n",
    "    x1 = (x0 + 1).clamp(0, W - 1)\n",
    "    y0 = torch.floor(y).long().clamp(0, H - 1)\n",
    "    y1 = (y0 + 1).clamp(0, H - 1)\n",
    "\n",
    "    Ia = depth_map[y0, x0]\n",
    "    Ib = depth_map[y1, x0]\n",
    "    Ic = depth_map[y0, x1]\n",
    "    Id = depth_map[y1, x1]\n",
    "\n",
    "    wa = (x1.float() - x) * (y1.float() - y)\n",
    "    wb = (x1.float() - x) * (y - y0.float())\n",
    "    wc = (x - x0.float()) * (y1.float() - y)\n",
    "    wd = (x - x0.float()) * (y - y0.float())\n",
    "\n",
    "    sampled_depths = wa * Ia + wb * Ib + wc * Ic + wd * Id\n",
    "\n",
    "    return sampled_depths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc7e68",
   "metadata": {},
   "source": [
    "# Batched ultilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4eda14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "# ---- helpers (slightly cleaned) ----\n",
    "def _pixel_grid(H, W, device, dtype):\n",
    "    # Returns pixel coords u (x) and v (y) shaped (H, W)\n",
    "    v, u = torch.meshgrid(\n",
    "        torch.arange(H, device=device, dtype=dtype),\n",
    "        torch.arange(W, device=device, dtype=dtype),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    return u, v  # u->x, v->y\n",
    "\n",
    "def batched_ray_dirs(intrinsics, H, W):\n",
    "    \"\"\"\n",
    "    intrinsics: (B, V, 3, 3)\n",
    "    returns: dirs (B, V, H, W, 3) normalized camera ray directions\n",
    "    \"\"\"\n",
    "    device = intrinsics.device\n",
    "    dtype = intrinsics.dtype\n",
    "    B, V = intrinsics.shape[0], intrinsics.shape[1]\n",
    "\n",
    "    u, v = _pixel_grid(H, W, device, dtype)   # (H, W) u=x, v=y\n",
    "    u = u.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    v = v.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "\n",
    "    fx = intrinsics[..., 0, 0].unsqueeze(-1).unsqueeze(-1)  # (B,V,1,1)\n",
    "    fy = intrinsics[..., 1, 1].unsqueeze(-1).unsqueeze(-1)\n",
    "    cx = intrinsics[..., 0, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "    cy = intrinsics[..., 1, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    dirs_x = (u - cx) / (fx + EPS)   # (B,V,H,W)\n",
    "    dirs_y = (v - cy) / (fy + EPS)\n",
    "    dirs_z = torch.ones_like(dirs_x)\n",
    "\n",
    "    dirs = torch.stack([dirs_x, dirs_y, dirs_z], dim=-1)  # (B,V,H,W,3)\n",
    "    norm = torch.norm(dirs, dim=-1, keepdim=True)\n",
    "    dirs = dirs / (norm + EPS)\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def batched_depth_to_camera(depth, ray_dirs):\n",
    "    # depth: (B,V,H,W), ray_dirs: (B,V,H,W,3)\n",
    "    return depth.unsqueeze(-1) * ray_dirs  # (B,V,H,W,3)\n",
    "\n",
    "\n",
    "def batched_camera_to_world(cam_pts, extrinsics):\n",
    "    \"\"\"\n",
    "    cam_pts: (B, V, H, W, 3)\n",
    "    extrinsics: (B, V, 4, 4)  (world -> camera)\n",
    "    returns: world_pts (B, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    B, V, H, W, _ = cam_pts.shape\n",
    "    R = extrinsics[..., :3, :3]         # (B,V,3,3) world->cam\n",
    "    t = extrinsics[..., :3, 3]          # (B,V,3)\n",
    "\n",
    "    R_inv = R.transpose(-1, -2)         # (B,V,3,3) cam->world\n",
    "    t_inv = -torch.matmul(R_inv, t.unsqueeze(-1)).squeeze(-1)  # (B,V,3)\n",
    "\n",
    "    pts = cam_pts.reshape(B, V, -1, 3)  # (B,V,HW,3)\n",
    "    pts_t = pts.permute(0,1,3,2)        # (B,V,3,HW)\n",
    "    world = torch.matmul(R_inv, pts_t) + t_inv.unsqueeze(-1)    # (B,V,3,HW)\n",
    "    world = world.permute(0,1,3,2).reshape(B, V, H, W, 3)\n",
    "    return world\n",
    "\n",
    "\n",
    "def batched_world_to_camera(world_pts, extrinsics):\n",
    "    \"\"\"\n",
    "    world_pts: (B, V, H, W, 3)\n",
    "    extrinsics: (B, V, 4, 4)  (world -> camera)\n",
    "    returns: cam_pts (B, V, H, W, 3)\n",
    "    \"\"\"\n",
    "    B, V, H, W, _ = world_pts.shape\n",
    "    R = extrinsics[..., :3, :3]    # (B,V,3,3)\n",
    "    t = extrinsics[..., :3, 3]     # (B,V,3)\n",
    "\n",
    "    pts = world_pts.reshape(B, V, -1, 3).permute(0,1,3,2)  # (B,V,3,HW)\n",
    "    cam = torch.matmul(R, pts) + t.unsqueeze(-1)           # (B,V,3,HW)\n",
    "    cam = cam.permute(0,1,3,2).reshape(B, V, H, W, 3)\n",
    "    return cam\n",
    "\n",
    "\n",
    "def batched_project_3d_to_2d(pts_3d, intrinsics):\n",
    "    \"\"\"\n",
    "    pts_3d: (B, V, H, W, 3) in camera coords\n",
    "    intrinsics: (B, V, 3, 3)\n",
    "    returns: uv (B, V, H, W, 2), z (B,V,H,W)\n",
    "    \"\"\"\n",
    "    x = pts_3d[..., 0]\n",
    "    y = pts_3d[..., 1]\n",
    "    z = pts_3d[..., 2].clamp(min=EPS)\n",
    "\n",
    "    z = torch.clamp(z, min=1e-4)\n",
    "\n",
    "    fx = intrinsics[..., 0, 0].unsqueeze(-1).unsqueeze(-1)\n",
    "    fy = intrinsics[..., 1, 1].unsqueeze(-1).unsqueeze(-1)\n",
    "    cx = intrinsics[..., 0, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "    cy = intrinsics[..., 1, 2].unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    u = fx * (x / z) + cx\n",
    "    v = fy * (y / z) + cy\n",
    "    uv = torch.stack([u, v], dim=-1)\n",
    "    return uv, z\n",
    "\n",
    "\n",
    "def normalize_uv_for_grid_sample(uv, H, W):\n",
    "    \"\"\"\n",
    "    uv: (..., 2) pixel coords with u in [0..W-1], v in [0..H-1]\n",
    "    returns: grid coords in [-1,1] last-dim order (x,y) for grid_sample\n",
    "    \"\"\"\n",
    "    u = uv[..., 0]\n",
    "    v = uv[..., 1]\n",
    "\n",
    "    nx = (u / (W - 1)) * 2 - 1\n",
    "    ny = (v / (H - 1)) * 2 - 1\n",
    "    return torch.stack([nx, ny], dim=-1)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# UNIT DETECTION & AUTO-SCALING\n",
    "# ================================================================\n",
    "def camera_centers_from_extrinsics(extrinsics):\n",
    "    \"\"\"\n",
    "    extrinsics: (B, V, 4, 4) world->camera\n",
    "    returns: camera centers in world coords (B, V, 3)\n",
    "    \"\"\"\n",
    "    R = extrinsics[..., :3, :3]   # (B,V,3,3)\n",
    "    t = extrinsics[..., :3, 3]    # (B,V,3)\n",
    "    R_inv = R.transpose(-1, -2)   # cam->world\n",
    "    cam_centers = -torch.matmul(R_inv, t.unsqueeze(-1)).squeeze(-1)  # (B,V,3)\n",
    "    return cam_centers\n",
    "\n",
    "\n",
    "def detect_and_fix_depth_unit(depth_batch, extrinsics, threshold_scale=10.0, apply_fix=True):\n",
    "    \"\"\"\n",
    "    Heuristic: if median depth >> median camera-translation magnitude,\n",
    "    likely depth is in mm (or cm). We scale down by 1000 or 100 accordingly.\n",
    "    Returns (depth_batch_scaled, scale_factor, did_scale_flag, med_scale, med_depth_mean, baseline_mean)\n",
    "    \"\"\"\n",
    "    device = depth_batch.device\n",
    "    B, V, H, W = depth_batch.shape\n",
    "\n",
    "    # stats on depth center\n",
    "    depth_center = depth_batch[:, 1]  # (B,H,W)\n",
    "    med_depth = torch.median(depth_center.reshape(B, -1), dim=1).values  # (B,)\n",
    "\n",
    "    # camera center distances (per sample)\n",
    "    centers = camera_centers_from_extrinsics(extrinsics)  # (B,V,3)\n",
    "    # baseline magnitude between src(0) and center(1)\n",
    "    baseline = torch.norm(centers[:, 0] - centers[:, 1], dim=-1)  # (B,)\n",
    "    # avoid zero baseline\n",
    "    baseline = baseline + 1e-6\n",
    "\n",
    "    scale_factors = med_depth / baseline  # if >> threshold -> likely depth in mm\n",
    "    med_scale = float(torch.median(scale_factors).item())\n",
    "\n",
    "    # Decide conversion\n",
    "    scale = 1.0\n",
    "    did_scale = False\n",
    "    if med_scale > threshold_scale:\n",
    "        # typical: depth in mm -> divide by 1000\n",
    "        if med_scale > 1000:\n",
    "            scale = 1.0 / 1000.0\n",
    "        else:\n",
    "            # try 1000 first\n",
    "            scale = 1.0 / 1000.0\n",
    "        if apply_fix:\n",
    "            depth_batch = depth_batch * scale\n",
    "            did_scale = True\n",
    "\n",
    "    return depth_batch, scale, did_scale, med_scale, float(med_depth.mean().item()), float(baseline.mean().item())\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# SAFE REPROJECTION (NO IN-PLACE OPS)\n",
    "# ================================================================\n",
    "def reprojection_pair_to_center_safe(\n",
    "    depth_batch, intrinsics, extrinsics,\n",
    "    center_idx=1, src_idx=0,\n",
    "    center_depth_override=None,\n",
    "    debug=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Safe reprojection: src -> center, no in-place ops on tensors that require grad.\n",
    "    center_depth_override: optional (B,H,W) tensor to use instead of depth_batch[:,center_idx]\n",
    "    Returns: X_src_world (B,H,W,3), X_center_world_reproj (B,H,W,3), valid (B,H,W), uv (B,H,W,2)\n",
    "    \"\"\"\n",
    "    B, V, H, W = depth_batch.shape\n",
    "    device = depth_batch.device\n",
    "    dtype = depth_batch.dtype\n",
    "\n",
    "    # 1) ray dirs\n",
    "    ray_dirs = batched_ray_dirs(intrinsics, H, W)  # (B,V,H,W,3)\n",
    "\n",
    "    # 2) src cam pts (use only src view)\n",
    "    ray_src = ray_dirs[:, src_idx]                 # (B,H,W,3)\n",
    "    depth_src = depth_batch[:, src_idx].unsqueeze(-1)   # (B,H,W,1)\n",
    "    depth_src = torch.clamp(depth_src, min=1e-4, max=1e6)\n",
    "    depth_src = torch.nan_to_num(depth_src, nan=1e-4, posinf=1e6, neginf=1e-4)\n",
    "    cam_pts_src = depth_src * ray_src              # (B,H,W,3)\n",
    "\n",
    "    # 3) world points src\n",
    "    X_src_world = batched_camera_to_world(cam_pts_src.unsqueeze(1), extrinsics[:, src_idx:src_idx+1]).squeeze(1)\n",
    "    X_src_world = torch.nan_to_num(X_src_world, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "    # 4) reproject src world into center camera coords (no in-place)\n",
    "    X_src_in_center_cam = batched_world_to_camera(X_src_world.unsqueeze(1), extrinsics[:, center_idx:center_idx+1]).squeeze(1)\n",
    "    x_xy = X_src_in_center_cam[..., :2]   # new view copies\n",
    "    z = X_src_in_center_cam[..., 2]\n",
    "    z_clamped = z.clamp(min=1e-4)\n",
    "    X_src_in_center_cam_clamped = torch.cat([x_xy, z_clamped.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # 5) project to pixels in center (uses a copy)\n",
    "    intr_c = intrinsics[:, center_idx:center_idx+1]\n",
    "    uv, z_proj = batched_project_3d_to_2d(X_src_in_center_cam_clamped.unsqueeze(1), intr_c)\n",
    "    uv = uv.squeeze(1); z_proj = z_proj.squeeze(1)\n",
    "\n",
    "    # clamp uv to image bounds without in-place writes\n",
    "    u = uv[..., 0].clamp(0.0, W - 1.0)\n",
    "    v = uv[..., 1].clamp(0.0, H - 1.0)\n",
    "    uv_clamped = torch.stack([u, v], dim=-1)\n",
    "    uv_clamped = torch.nan_to_num(uv_clamped, nan=0.0, posinf=W-1.0, neginf=0.0)\n",
    "\n",
    "    # 6) sample center depth at uv using either override or original depth\n",
    "    grid = normalize_uv_for_grid_sample(uv_clamped, H, W)  # safe normalization\n",
    "    if center_depth_override is None:\n",
    "        depth_center = depth_batch[:, center_idx].unsqueeze(1)  # (B,1,H,W)\n",
    "    else:\n",
    "        # center_depth_override: (B,H,W) -> convert to (B,1,H,W) for grid_sample\n",
    "        depth_center = center_depth_override.unsqueeze(1)\n",
    "\n",
    "    depth_center = torch.clamp(depth_center, min=1e-4, max=1e6)\n",
    "    sampled_center = F.grid_sample(depth_center, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "    sampled_center = torch.nan_to_num(sampled_center, nan=0.0, posinf=1e6, neginf=0.0).squeeze(1)\n",
    "\n",
    "    # 7) valid mask\n",
    "    in_bounds = (uv_clamped[...,0] >= 0) & (uv_clamped[...,0] <= (W - 1)) & (uv_clamped[...,1] >= 0) & (uv_clamped[...,1] <= (H - 1))\n",
    "    valid = in_bounds & (z_proj > 0) & (sampled_center > 0)\n",
    "\n",
    "    # 8) sample center ray dirs\n",
    "    ray_center = ray_dirs[:, center_idx]  # (B,H,W,3)\n",
    "    ray_center_t = ray_center.permute(0,3,1,2)\n",
    "    sampled_rays = F.grid_sample(ray_center_t, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "    sampled_rays = torch.nan_to_num(sampled_rays, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    sampled_rays = sampled_rays.permute(0,2,3,1)\n",
    "\n",
    "    # 9) backproject sampled center -> world\n",
    "    X_center_cam_reproj = sampled_center.unsqueeze(-1) * sampled_rays\n",
    "    X_center_world_reproj = batched_camera_to_world(X_center_cam_reproj.unsqueeze(1), extrinsics[:, center_idx:center_idx+1]).squeeze(1)\n",
    "    X_center_world_reproj = torch.nan_to_num(X_center_world_reproj, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[reproj debug SAFE] src={src_idx} B={B} H={H} W={W}\")\n",
    "        print(\" X_src_world minZ:\", float(X_src_world[...,2].min().item()))\n",
    "        print(\" X_center_world_reproj minZ:\", float(X_center_world_reproj[...,2].min().item()))\n",
    "        print(\" uv min/max:\", float(uv_clamped[...,0].min().item()), float(uv_clamped[...,0].max().item()),\n",
    "              float(uv_clamped[...,1].min().item()), float(uv_clamped[...,1].max().item()))\n",
    "        print(\" sampled_center min/max:\", float(sampled_center.min().item()), float(sampled_center.max().item()))\n",
    "        print(\" valid_count:\", int(valid.sum().item()))\n",
    "\n",
    "    return X_src_world, X_center_world_reproj, valid, uv_clamped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7c65f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 80, 3])\n",
      "torch.Size([2, 64, 80, 3])\n",
      "torch.Size([2, 64, 80])\n",
      "torch.Size([2, 64, 80, 2])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# quick test\n",
    "B, V, H, W = 2, 3, 64, 80\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "depth = torch.rand(B, V, H, W, device=device, dtype=dtype) * 10.0 + 0.1  # positive depths\n",
    "intr = torch.eye(3, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0).repeat(B, V, 1, 1)\n",
    "# tweak fx,fy,cx,cy for visual\n",
    "intr[..., 0, 0] = 60.0\n",
    "intr[..., 1, 1] = 60.0\n",
    "intr[..., 0, 2] = W / 2.0\n",
    "intr[..., 1, 2] = H / 2.0\n",
    "\n",
    "# build extrinsics: identity cameras at different x offsets\n",
    "extr = torch.eye(4, device=device, dtype=dtype).unsqueeze(0).unsqueeze(0).repeat(B, V, 1, 1)\n",
    "# shift cameras along x\n",
    "for v in range(V):\n",
    "    extr[:, v, 0, 3] = (v - 1) * 0.2  # small baseline\n",
    "\n",
    "X_src_world, X_center_world_reproj, valid, uv = reprojection_pair_to_center(\n",
    "    depth, intr, extr, center_idx=1, src_idx=0\n",
    ")\n",
    "\n",
    "print(X_src_world.shape)         # (B,H,W,3)\n",
    "print(X_center_world_reproj.shape)  # (B,H,W,3)\n",
    "print(valid.shape)               # (B,H,W)\n",
    "print(uv.shape)                  # (B,H,W,2)\n",
    "print(valid.float().mean().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6153eb",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Wrapper: DepthAnything forward on RGB batch\n",
    "# -------------------------\n",
    "def depth_anything_forward(rgb_batch, depth_anything_model):\n",
    "    \"\"\"\n",
    "    rgb_batch: (B, V, 3, H, W) in [0, 1] float32\n",
    "    Returns: depth_init_all (B, V, H, W) normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    device = rgb_batch.device\n",
    "    B, V, C, H, W = rgb_batch.shape\n",
    "    \n",
    "    depth_init_all = []\n",
    "    \n",
    "    for b in range(B):\n",
    "        depth_batch_b = []\n",
    "        for v in range(V):\n",
    "            # Extract single RGB frame: (3, H, W) -> PIL Image\n",
    "            rgb_frame = rgb_batch[b, v].permute(1, 2, 0)  # (H, W, 3)\n",
    "            rgb_frame_np = (rgb_frame * 255).clamp(0, 255).byte().cpu().numpy()\n",
    "            \n",
    "            # Convert numpy to PIL Image\n",
    "            pil_image = Image.fromarray(rgb_frame_np.astype(np.uint8), mode='RGB')\n",
    "            \n",
    "            # Run Depth Anything inference\n",
    "            with torch.no_grad():\n",
    "                result = depth_anything_model(pil_image)\n",
    "            \n",
    "            # result is a dict with 'depth' key (PIL Image)\n",
    "            depth_pred = result['depth']\n",
    "            \n",
    "            # Convert PIL Image to numpy\n",
    "            if isinstance(depth_pred, Image.Image):\n",
    "                depth_pred = np.array(depth_pred)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            depth_pred = depth_pred.astype(np.float32)\n",
    "            depth_min, depth_max = depth_pred.min(), depth_pred.max()\n",
    "            if depth_max > depth_min:\n",
    "                depth_pred = (depth_pred - depth_min) / (depth_max - depth_min)\n",
    "            else:\n",
    "                depth_pred = np.ones_like(depth_pred) * 0.5\n",
    "            \n",
    "            # Convert back to tensor\n",
    "            depth_pred = torch.from_numpy(depth_pred).float().to(device)  # (H, W)\n",
    "            depth_batch_b.append(depth_pred)\n",
    "        \n",
    "        depth_init_all.append(torch.stack(depth_batch_b, dim=0))  # (V, H, W)\n",
    "    \n",
    "    depth_init_all = torch.stack(depth_init_all, dim=0)  # (B, V, H, W)\n",
    "    return depth_init_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f1917e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Loss Functions with Huber\n",
    "# -------------------------\n",
    "\n",
    "def affine_align_depth(depth_init, gt_depth, mask, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute affine alignment: s, t such that s*depth_init + t â‰ˆ gt_depth\n",
    "    Returns aligned depth and scale factors\n",
    "    \n",
    "    depth_init: (B, 1, H, W)\n",
    "    gt_depth:   (B, 1, H, W)\n",
    "    mask:       (B, 1, H, W) boolean\n",
    "    \"\"\"\n",
    "    B = depth_init.shape[0]\n",
    "    s_list = []\n",
    "    t_list = []\n",
    "    depth_aligned = torch.zeros_like(depth_init)\n",
    "    \n",
    "    for b in range(B):\n",
    "        m = mask[b, 0].reshape(-1)\n",
    "        if m.sum() < 10:\n",
    "            # Not enough valid pixels, use identity\n",
    "            s_list.append(1.0)\n",
    "            t_list.append(0.0)\n",
    "            depth_aligned[b] = depth_init[b]\n",
    "            continue\n",
    "        \n",
    "        d = depth_init[b, 0].reshape(-1)[m]  # (N,)\n",
    "        g = gt_depth[b, 0].reshape(-1)[m]    # (N,)\n",
    "        \n",
    "        # Least squares: minimize || s*d + t - g ||^2\n",
    "        # A = [d, 1], x = [s, t]^T, b = g\n",
    "        A = torch.stack([d, torch.ones_like(d)], dim=1)  # (N, 2)\n",
    "        g_col = g.unsqueeze(1)  # (N, 1)\n",
    "        \n",
    "        # Solve using lstsq (stable)\n",
    "        try:\n",
    "            x = torch.linalg.lstsq(A, g_col).solution  # (2, 1)\n",
    "            s, t = float(x[0].item()), float(x[1].item())\n",
    "        except:\n",
    "            s, t = 1.0, 0.0\n",
    "        \n",
    "        s_list.append(s)\n",
    "        t_list.append(t)\n",
    "        depth_aligned[b, 0] = depth_init[b, 0] * s + t\n",
    "    \n",
    "    return depth_aligned, s_list, t_list\n",
    "\n",
    "\n",
    "def edge_aware_smoothness(depth, rgb):\n",
    "    \"\"\"\n",
    "    Edge-aware smoothness loss: smooth depth except at RGB edges\n",
    "    depth: (B, 1, H, W)\n",
    "    rgb: (B, 3, H, W)\n",
    "    \"\"\"\n",
    "    grad_depth_x = torch.abs(depth[:, :, :, :-1] - depth[:, :, :, 1:])\n",
    "    grad_depth_y = torch.abs(depth[:, :, :-1, :] - depth[:, :, 1:, :])\n",
    "    \n",
    "    grad_rgb_x = torch.mean(torch.abs(rgb[:, :, :, :-1] - rgb[:, :, :, 1:]), dim=1, keepdim=True)\n",
    "    grad_rgb_y = torch.mean(torch.abs(rgb[:, :, :-1, :] - rgb[:, :, 1:, :]), dim=1, keepdim=True)\n",
    "    \n",
    "    L_edge_x = torch.mean(grad_depth_x * torch.exp(-grad_rgb_x))\n",
    "    L_edge_y = torch.mean(grad_depth_y * torch.exp(-grad_rgb_y))\n",
    "    \n",
    "    return L_edge_x + L_edge_y\n",
    "\n",
    "\n",
    "def normals_from_depth(depth, intrinsics):\n",
    "    \"\"\"\n",
    "    Compute surface normals from depth\n",
    "    depth: (B, 1, H, W)\n",
    "    intrinsics: (B, V, 3, 3) - use first V=0 for center\n",
    "    returns: normals (B, 1, H, W, 3) normalized\n",
    "    \"\"\"\n",
    "    B = depth.shape[0]\n",
    "    intr_center = intrinsics[:, 0]  # (B, 3, 3)\n",
    "    \n",
    "    fx = intr_center[:, 0, 0].view(B, 1, 1, 1)\n",
    "    fy = intr_center[:, 1, 1].view(B, 1, 1, 1)\n",
    "    \n",
    "    # Finite difference for gradients\n",
    "    dz_dx = (depth[:, :, :, 1:] - depth[:, :, :, :-1]) / (fx + 1e-6)\n",
    "    dz_dy = (depth[:, :, 1:, :] - depth[:, :, :-1, :]) / (fy + 1e-6)\n",
    "    \n",
    "    # Pad to original size\n",
    "    dz_dx = torch.cat([dz_dx, dz_dx[:, :, :, -1:]], dim=3)\n",
    "    dz_dy = torch.cat([dz_dy, dz_dy[:, :, -1:, :]], dim=2)\n",
    "    \n",
    "    # Normal: (-dz/dx, -dz/dy, 1)\n",
    "    nx = -dz_dx\n",
    "    ny = -dz_dy\n",
    "    nz = torch.ones_like(nx)\n",
    "    \n",
    "    normals = torch.stack([nx, ny, nz], dim=-1)  # (B, 1, H, W, 3)\n",
    "    norm = torch.norm(normals, dim=-1, keepdim=True) + 1e-6\n",
    "    normals = normals / norm\n",
    "    \n",
    "    return normals\n",
    "\n",
    "\n",
    "def normal_smoothness_loss(normals):\n",
    "    \"\"\"\n",
    "    Encourage smooth normals (small differences between neighbors)\n",
    "    normals: (B, 1, H, W, 3)\n",
    "    \"\"\"\n",
    "    n = normals\n",
    "    L_x = torch.mean(torch.abs(n[:, :, :, :-1, :] - n[:, :, :, 1:, :]))\n",
    "    L_y = torch.mean(torch.abs(n[:, :, :-1, :, :] - n[:, :, 1:, :, :]))\n",
    "    return L_x + L_y\n",
    "\n",
    "\n",
    "def masked_l1_huber(pred, gt, mask, eps=1e-6, beta=0.1):\n",
    "    \"\"\"\n",
    "    Masked Huber loss (SmoothL1)\n",
    "    pred: (B, 1, H, W)\n",
    "    gt: (B, 1, H, W)\n",
    "    mask: (B, 1, H, W) boolean\n",
    "    beta: Huber transition threshold\n",
    "    \"\"\"\n",
    "    valid = mask.float()\n",
    "    \n",
    "    # Clamp both pred and gt to avoid extreme values\n",
    "    pred_safe = torch.clamp(pred, min=0.0, max=1e4)\n",
    "    gt_safe = torch.clamp(gt, min=0.0, max=1e4)\n",
    "    \n",
    "    huber = torch.nn.SmoothL1Loss(reduction='none', beta=beta)\n",
    "    loss = huber(pred_safe, gt_safe)\n",
    "    \n",
    "    # Safety: remove any NaN/Inf\n",
    "    loss = torch.nan_to_num(loss, nan=0.0, posinf=1e2, neginf=0.0)\n",
    "    loss = torch.clamp(loss, min=0.0, max=1e2)\n",
    "    \n",
    "    masked_loss = (loss * valid).sum() / (valid.sum() + eps)\n",
    "    masked_loss = torch.nan_to_num(masked_loss, nan=0.0, posinf=1e2, neginf=0.0)\n",
    "    \n",
    "    return masked_loss\n",
    "\n",
    "\n",
    "def train_one_epoch(model, depth_anything_model, dataloader, optimizer, device, \n",
    "                    use_gt_for_loss=True, lambda_mv=0.001, lambda_init=0.1, lambda_edge=0.05, \n",
    "                    lambda_norm=0.1, num_steps=1):\n",
    "    \"\"\"\n",
    "    Single training epoch with multi-view consistency and Huber loss\n",
    "    \n",
    "    Args:\n",
    "        model: UNet (7 in, 1 out)\n",
    "        depth_anything_model: Frozen depth estimation pipeline\n",
    "        dataloader: GTAV loader\n",
    "        optimizer: Adam\n",
    "        device: cuda/cpu\n",
    "        use_gt_for_loss: use GT depth for L_gt\n",
    "        lambda_init: weight for L_init (smoothness)\n",
    "        lambda_edge: weight for L_edge (edge-aware)\n",
    "        lambda_norm: weight for L_norm (normal smoothness)\n",
    "        num_steps: number of batches to process\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    GT_INVALID_THRESH = 100.0\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "        \n",
    "        batch_rgb = batch['rgb'].to(device)  # (B, V, 3, H, W)\n",
    "        batch_depth = batch['depth'].to(device)  # (B, V, H, W)\n",
    "        batch_intr = batch['intrinsic'].to(device)  # (B, V, 3, 3)\n",
    "        batch_extr = batch['extrinsic'].to(device)  # (B, V, 4, 4)\n",
    "        \n",
    "        B, V, _, H, W = batch_rgb.shape\n",
    "        center_idx = V // 2\n",
    "        \n",
    "        print(f\"\\n[Step {step}] B={B}, V={V}, H={H}, W={W}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 1) DepthAnything inference (frozen)\n",
    "        # ============================================\n",
    "        depth_init_all = depth_anything_forward(batch_rgb, depth_anything_model)  # (B, V, H, W)\n",
    "        depth_init_all = torch.clamp(depth_init_all, min=1e-4, max=1.0)\n",
    "        print(f\"[DepthAnything] min={depth_init_all.min():.4f}, max={depth_init_all.max():.4f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 2) Detect & fix depth unit mismatch\n",
    "        # ============================================\n",
    "        depth_init_scaled, scale, did_scale, med_scale, med_depth, baseline = \\\n",
    "            detect_and_fix_depth_unit(batch_depth, batch_extr, threshold_scale=10.0, apply_fix=True)\n",
    "        print(f\"[Unit detection] scale={scale:.4f}, did_scale={did_scale}, med_scale={med_scale:.2f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 3) Prepare center view depth\n",
    "        # ============================================\n",
    "        depth_init_center = depth_init_all[:, center_idx].unsqueeze(1)  # (B, 1, H, W)\n",
    "        gt_center = depth_init_scaled[:, center_idx].unsqueeze(1)  # (B, 1, H, W)\n",
    "        \n",
    "        # GT mask: finite, positive, not too large\n",
    "        gt_valid = (~torch.isinf(gt_center)) & (gt_center < GT_INVALID_THRESH) & (gt_center > 0)\n",
    "        n_valid_gt = gt_valid.float().sum().item()\n",
    "        print(f\"[GT mask] valid_pixels={n_valid_gt:.0f} / {B*H*W}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 4) Affine alignment: scale init_depth to match GT scale\n",
    "        # ============================================\n",
    "        try:\n",
    "            depth_init_aligned, s_scales, t_offsets = affine_align_depth(depth_init_center, gt_center, gt_valid)\n",
    "            print(f\"[Affine align] scales={[f'{s:.2f}' for s in s_scales]}, offsets={[f'{t:.2f}' for t in t_offsets]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Affine align ERROR] {e}, using identity (s=1, t=0)\")\n",
    "            depth_init_aligned = depth_init_center.clone()\n",
    "            s_scales = [1.0] * B\n",
    "            t_offsets = [0.0] * B\n",
    "        \n",
    "        # ============================================\n",
    "        # 5) Prepare input to UNet (use aligned depth)\n",
    "        # ============================================\n",
    "        ray_dirs_batch = batched_ray_dirs(batch_intr, H, W)  # (B, V, H, W, 3)\n",
    "        \n",
    "        rgb_center = batch_rgb[:, center_idx]  # (B, 3, H, W)\n",
    "        ray_dirs_center = ray_dirs_batch[:, center_idx]  # (B, H, W, 3)\n",
    "        ray_dirs_center = ray_dirs_center.permute(0, 3, 1, 2)  # (B, 3, H, W)\n",
    "        \n",
    "        # Concatenate input: RGB(3) + depth_init_aligned(1) + ray_dirs(3) = 7 channels\n",
    "        model_input = torch.cat([rgb_center, depth_init_aligned, ray_dirs_center], dim=1)  # (B, 7, H, W)\n",
    "        \n",
    "        # ============================================\n",
    "        # 6) Forward pass\n",
    "        # ============================================\n",
    "        depth_delta = model(model_input)  # (B, 1, H, W)\n",
    "        pred_depth_center = depth_init_aligned + depth_delta\n",
    "        pred_depth_center = torch.clamp(pred_depth_center, min=1e-4, max=1e6)\n",
    "        \n",
    "        print(f\"[Prediction] pred_depth min={pred_depth_center.min():.4f}, max={pred_depth_center.max():.4f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 7) Multi-view reprojection loss\n",
    "        # ============================================\n",
    "        mv_loss = 0.0\n",
    "        for src_idx in [0, 2]:  # skip center_idx=1\n",
    "            X_src_world, X_center_world_reproj, valid, _ = reprojection_pair_to_center_safe(\n",
    "                depth_init_scaled, batch_intr, batch_extr,\n",
    "                center_idx=center_idx, src_idx=src_idx,\n",
    "                center_depth_override=pred_depth_center.squeeze(1),\n",
    "                debug=(step == 0)\n",
    "            )\n",
    "            \n",
    "            # 3D point reprojection error in world space\n",
    "            diff_3d = X_src_world - X_center_world_reproj  # (B, H, W, 3)\n",
    "            error_3d = torch.norm(diff_3d, dim=-1)  # (B, H, W)\n",
    "            valid_f = valid.float()\n",
    "            \n",
    "            loss_pair = (error_3d * valid_f).sum() / (valid_f.sum() + EPS)\n",
    "            mv_loss = mv_loss + loss_pair\n",
    "            \n",
    "            n_valid = valid_f.sum().item()\n",
    "            print(f\"  src={src_idx}: loss_pair={loss_pair:.4f}, valid_pixels={n_valid:.0f}\")\n",
    "        \n",
    "        mv_loss = mv_loss / 2.0  # average over 2 pairs\n",
    "        print(f\"[MV Loss] {mv_loss:.4f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 8) GT supervised loss (Huber, now on aligned scale)\n",
    "        # ============================================\n",
    "        L_gt = 0.0\n",
    "        if use_gt_for_loss and n_valid_gt > 10:\n",
    "            # Apply the same affine scale/offset to pred as we did to init\n",
    "            # This brings pred into GT's metric space\n",
    "            s_tensor = torch.tensor(s_scales, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "            t_tensor = torch.tensor(t_offsets, device=device, dtype=pred_depth_center.dtype).view(B, 1, 1, 1)\n",
    "            pred_aligned = pred_depth_center * s_tensor + t_tensor\n",
    "            \n",
    "            print(f\"[Affine applied to pred] s={s_scales}, t={t_offsets}\")\n",
    "            print(f\"  pred_aligned min/max: {pred_aligned.min():.4f}, {pred_aligned.max():.4f}\")\n",
    "            print(f\"  gt_center min/max: {gt_center.min():.4f}, {gt_center.max():.4f}\")\n",
    "            \n",
    "            L_gt = masked_l1_huber(pred_aligned, gt_center, gt_valid, beta=0.1)\n",
    "            L_gt = torch.nan_to_num(L_gt, nan=0.0, posinf=1e2, neginf=0.0)\n",
    "            print(f\"[L_gt] {L_gt:.4f} (Huber, affine-aligned)\")\n",
    "\n",
    "        \n",
    "        # ============================================\n",
    "        # 9) Initial depth smoothness (edge-aware)\n",
    "        # ============================================\n",
    "        L_init = edge_aware_smoothness(depth_init_aligned, rgb_center)\n",
    "        print(f\"[L_init (edge_smooth)] {L_init:.4f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 10) Edge-aware smoothness on prediction\n",
    "        # ============================================\n",
    "        L_edge = edge_aware_smoothness(pred_depth_center, rgb_center)\n",
    "        print(f\"[L_edge] {L_edge:.4f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 11) Normal smoothness\n",
    "        # ============================================\n",
    "        normals = normals_from_depth(pred_depth_center, batch_intr)\n",
    "        L_norm = normal_smoothness_loss(normals)\n",
    "        print(f\"[L_norm] {L_norm:.4f}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 12) Combined loss\n",
    "        # ============================================\n",
    "        total_loss = lambda_mv*mv_loss + 1.0*L_gt + lambda_init*L_init + lambda_edge*L_edge + lambda_norm*L_norm\n",
    "        print(f\"[Total Loss] {total_loss:.4f} (mv_weighted={lambda_mv*mv_loss:.4f})\")\n",
    "        \n",
    "        # ============================================\n",
    "        # 13) Backward pass\n",
    "        # ============================================\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"[Backward] completed, gradients updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Using] Existing Depth Anything pipeline\n",
      "[Device] Using: cuda\n",
      "\n",
      "================================================================================\n",
      "Starting training with Depth Anything Small model - 5 epochs\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EPOCH 1/5\n",
      "================================================================================\n",
      "\n",
      "[Step 0] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 0] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=266750000.00\n",
      "[GT mask] valid_pixels=1399672 / 1555200\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=266750000.00\n",
      "[GT mask] valid_pixels=1399672 / 1555200\n",
      "[Affine align] scales=['-1.51', '-1.21', '-1.83'], offsets=['0.86', '0.88', '1.37']\n",
      "[Affine align] scales=['-1.51', '-1.21', '-1.83'], offsets=['0.86', '0.88', '1.37']\n",
      "[Prediction] pred_depth min=0.0001, max=1.5878\n",
      "[Prediction] pred_depth min=0.0001, max=1.5878\n",
      "[reproj debug SAFE] src=0 B=3 H=540 W=960\n",
      " X_src_world minZ: -201601.421875\n",
      " X_center_world_reproj minZ: 215.28631591796875\n",
      " uv min/max: 0.0 925.4099731445312 0.0 390.44287109375\n",
      " sampled_center min/max: 0.6472070813179016 1.5801219940185547\n",
      " valid_count: 1555200\n",
      "  src=0: loss_pair=105770.5312, valid_pixels=1555200\n",
      "[reproj debug SAFE] src=2 B=3 H=540 W=960\n",
      "[reproj debug SAFE] src=0 B=3 H=540 W=960\n",
      " X_src_world minZ: -201601.421875\n",
      " X_center_world_reproj minZ: 215.28631591796875\n",
      " uv min/max: 0.0 925.4099731445312 0.0 390.44287109375\n",
      " sampled_center min/max: 0.6472070813179016 1.5801219940185547\n",
      " valid_count: 1555200\n",
      "  src=0: loss_pair=105770.5312, valid_pixels=1555200\n",
      "[reproj debug SAFE] src=2 B=3 H=540 W=960\n",
      " X_src_world minZ: -198060.640625\n",
      " X_center_world_reproj minZ: 215.37425231933594\n",
      " uv min/max: 0.0 958.9989013671875 0.0 534.135009765625\n",
      " sampled_center min/max: 9.999998292187229e-05 1.5872406959533691\n",
      " valid_count: 1555200\n",
      "  src=2: loss_pair=100005.4609, valid_pixels=1555200\n",
      "[MV Loss] 102888.0000\n",
      "[Affine applied to pred] s=[-1.5061461925506592, -1.2107250690460205, -1.8311601877212524], t=[0.861095666885376, 0.8837174773216248, 1.3722199201583862]\n",
      "  pred_aligned min/max: -1.5354, 1.3720\n",
      "  gt_center min/max: 0.0333, inf\n",
      "[L_gt] 0.5834 (Huber, affine-aligned)\n",
      " X_src_world minZ: -198060.640625\n",
      " X_center_world_reproj minZ: 215.37425231933594\n",
      " uv min/max: 0.0 958.9989013671875 0.0 534.135009765625\n",
      " sampled_center min/max: 9.999998292187229e-05 1.5872406959533691\n",
      " valid_count: 1555200\n",
      "  src=2: loss_pair=100005.4609, valid_pixels=1555200\n",
      "[MV Loss] 102888.0000\n",
      "[Affine applied to pred] s=[-1.5061461925506592, -1.2107250690460205, -1.8311601877212524], t=[0.861095666885376, 0.8837174773216248, 1.3722199201583862]\n",
      "  pred_aligned min/max: -1.5354, 1.3720\n",
      "  gt_center min/max: 0.0333, inf\n",
      "[L_gt] 0.5834 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0037\n",
      "[L_init (edge_smooth)] 0.0037\n",
      "[L_edge] 0.0043\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 103.4720 (mv_weighted=102.8880)\n",
      "[L_edge] 0.0043\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 103.4720 (mv_weighted=102.8880)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 1] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 1] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=310000000.00\n",
      "[GT mask] valid_pixels=1222771 / 1555200\n",
      "[Affine align] scales=['-1.24', '-2.30', '-2.66'], offsets=['0.84', '0.86', '1.81']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=310000000.00\n",
      "[GT mask] valid_pixels=1222771 / 1555200\n",
      "[Affine align] scales=['-1.24', '-2.30', '-2.66'], offsets=['0.84', '0.86', '1.81']\n",
      "[Prediction] pred_depth min=0.0001, max=2.0287\n",
      "[Prediction] pred_depth min=0.0001, max=2.0287\n",
      "  src=0: loss_pair=214369.7656, valid_pixels=1555200\n",
      "  src=0: loss_pair=214369.7656, valid_pixels=1555200\n",
      "  src=2: loss_pair=213753.4688, valid_pixels=1555200\n",
      "[MV Loss] 214061.6250\n",
      "[Affine applied to pred] s=[-1.238256812095642, -2.295192003250122, -2.6571245193481445], t=[0.84295654296875, 0.8590178489685059, 1.8102558851242065]\n",
      "  pred_aligned min/max: -3.5803, 1.8100\n",
      "  gt_center min/max: 0.0503, inf\n",
      "[L_gt] 0.5878 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0058\n",
      "  src=2: loss_pair=213753.4688, valid_pixels=1555200\n",
      "[MV Loss] 214061.6250\n",
      "[Affine applied to pred] s=[-1.238256812095642, -2.295192003250122, -2.6571245193481445], t=[0.84295654296875, 0.8590178489685059, 1.8102558851242065]\n",
      "  pred_aligned min/max: -3.5803, 1.8100\n",
      "  gt_center min/max: 0.0503, inf\n",
      "[L_gt] 0.5878 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0058\n",
      "[L_edge] 0.0059\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 214.6503 (mv_weighted=214.0616)\n",
      "[L_edge] 0.0059\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 214.6503 (mv_weighted=214.0616)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 2] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 2] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=460250016.00\n",
      "[GT mask] valid_pixels=1095345 / 1555200\n",
      "[Affine align] scales=['-6.49', '-3.24', '-1.88'], offsets=['3.15', '2.20', '1.55']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=460250016.00\n",
      "[GT mask] valid_pixels=1095345 / 1555200\n",
      "[Affine align] scales=['-6.49', '-3.24', '-1.88'], offsets=['3.15', '2.20', '1.55']\n",
      "[Prediction] pred_depth min=0.0001, max=3.3804\n",
      "[Prediction] pred_depth min=0.0001, max=3.3804\n",
      "  src=0: loss_pair=322852.7188, valid_pixels=1555200\n",
      "  src=0: loss_pair=322852.7188, valid_pixels=1555200\n",
      "  src=2: loss_pair=295687.8438, valid_pixels=1555200\n",
      "[MV Loss] 309270.2812\n",
      "[Affine applied to pred] s=[-6.4871931076049805, -3.241272449493408, -1.8799405097961426], t=[3.152184009552002, 2.2013189792633057, 1.5489180088043213]\n",
      "  pred_aligned min/max: -18.7772, 3.1515\n",
      "  gt_center min/max: 0.0789, inf\n",
      "[L_gt] 0.9751 (Huber, affine-aligned)\n",
      "  src=2: loss_pair=295687.8438, valid_pixels=1555200\n",
      "[MV Loss] 309270.2812\n",
      "[Affine applied to pred] s=[-6.4871931076049805, -3.241272449493408, -1.8799405097961426], t=[3.152184009552002, 2.2013189792633057, 1.5489180088043213]\n",
      "  pred_aligned min/max: -18.7772, 3.1515\n",
      "  gt_center min/max: 0.0789, inf\n",
      "[L_gt] 0.9751 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0094\n",
      "[L_edge] 0.0093\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 310.2469 (mv_weighted=309.2703)\n",
      "[L_init (edge_smooth)] 0.0094\n",
      "[L_edge] 0.0093\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 310.2469 (mv_weighted=309.2703)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 3] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 3] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=108.95\n",
      "[GT mask] valid_pixels=1270762 / 1555200\n",
      "[Affine align] scales=['-0.64', '-2.72', '-1.90'], offsets=['0.53', '1.85', '1.36']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=108.95\n",
      "[GT mask] valid_pixels=1270762 / 1555200\n",
      "[Affine align] scales=['-0.64', '-2.72', '-1.90'], offsets=['0.53', '1.85', '1.36']\n",
      "[Prediction] pred_depth min=0.0001, max=2.0816\n",
      "[Prediction] pred_depth min=0.0001, max=2.0816\n",
      "  src=0: loss_pair=216340.7812, valid_pixels=1555200\n",
      "  src=0: loss_pair=216340.7812, valid_pixels=1555200\n",
      "  src=2: loss_pair=182894.6250, valid_pixels=1555200\n",
      "[MV Loss] 199617.7031\n",
      "[Affine applied to pred] s=[-0.636037290096283, -2.7175164222717285, -1.9016287326812744], t=[0.5254946947097778, 1.8517968654632568, 1.3572425842285156]\n",
      "  pred_aligned min/max: -3.8050, 1.8515\n",
      "  gt_center min/max: 0.0702, inf\n",
      "[L_gt] 0.6998 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0045\n",
      "[L_edge] 0.0046\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 200.3182 (mv_weighted=199.6177)\n",
      "  src=2: loss_pair=182894.6250, valid_pixels=1555200\n",
      "[MV Loss] 199617.7031\n",
      "[Affine applied to pred] s=[-0.636037290096283, -2.7175164222717285, -1.9016287326812744], t=[0.5254946947097778, 1.8517968654632568, 1.3572425842285156]\n",
      "  pred_aligned min/max: -3.8050, 1.8515\n",
      "  gt_center min/max: 0.0702, inf\n",
      "[L_gt] 0.6998 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0045\n",
      "[L_edge] 0.0046\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 200.3182 (mv_weighted=199.6177)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 4] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 4] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=509.34\n",
      "[GT mask] valid_pixels=1021990 / 1555200\n",
      "[Affine align] scales=['-3.16', '-4.02', '-2.50'], offsets=['2.08', '2.53', '1.37']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=509.34\n",
      "[GT mask] valid_pixels=1021990 / 1555200\n",
      "[Affine align] scales=['-3.16', '-4.02', '-2.50'], offsets=['2.08', '2.53', '1.37']\n",
      "[Prediction] pred_depth min=0.0001, max=2.7666\n",
      "[Prediction] pred_depth min=0.0001, max=2.7666\n",
      "  src=0: loss_pair=341530.8750, valid_pixels=1555200\n",
      "  src=2: loss_pair=342856.2188, valid_pixels=1555200\n",
      "[MV Loss] 342193.5625\n",
      "[Affine applied to pred] s=[-3.160980463027954, -4.023022651672363, -2.5027146339416504], t=[2.0817630290985107, 2.5323550701141357, 1.3676966428756714]\n",
      "  pred_aligned min/max: -8.5978, 2.5320\n",
      "  gt_center min/max: 0.0529, inf\n",
      "[L_gt] 1.1801 (Huber, affine-aligned)\n",
      "  src=0: loss_pair=341530.8750, valid_pixels=1555200\n",
      "  src=2: loss_pair=342856.2188, valid_pixels=1555200\n",
      "[MV Loss] 342193.5625\n",
      "[Affine applied to pred] s=[-3.160980463027954, -4.023022651672363, -2.5027146339416504], t=[2.0817630290985107, 2.5323550701141357, 1.3676966428756714]\n",
      "  pred_aligned min/max: -8.5978, 2.5320\n",
      "  gt_center min/max: 0.0529, inf\n",
      "[L_gt] 1.1801 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0096\n",
      "[L_edge] 0.0089\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 343.3751 (mv_weighted=342.1936)\n",
      "[L_init (edge_smooth)] 0.0096\n",
      "[L_edge] 0.0089\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 343.3751 (mv_weighted=342.1936)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "================================================================================\n",
      "EPOCH 2/5\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EPOCH 2/5\n",
      "================================================================================\n",
      "\n",
      "[Step 0] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 0] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=226.92\n",
      "[GT mask] valid_pixels=1314225 / 1555200\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=226.92\n",
      "[GT mask] valid_pixels=1314225 / 1555200\n",
      "[Affine align] scales=['-2.86', '-1.13', '-2.25'], offsets=['1.91', '0.79', '1.55']\n",
      "[Affine align] scales=['-2.86', '-1.13', '-2.25'], offsets=['1.91', '0.79', '1.55']\n",
      "[Prediction] pred_depth min=0.0001, max=2.1521\n",
      "[Prediction] pred_depth min=0.0001, max=2.1521\n",
      "[reproj debug SAFE] src=0 B=3 H=540 W=960\n",
      " X_src_world minZ: -123057.59375\n",
      " X_center_world_reproj minZ: 360.47802734375\n",
      " uv min/max: 0.0 959.0 0.0 539.0\n",
      " sampled_center min/max: 9.99999901978299e-05 2.133143424987793\n",
      " valid_count: 1555200\n",
      "  src=0: loss_pair=139971.0312, valid_pixels=1555200\n",
      "[reproj debug SAFE] src=0 B=3 H=540 W=960\n",
      " X_src_world minZ: -123057.59375\n",
      " X_center_world_reproj minZ: 360.47802734375\n",
      " uv min/max: 0.0 959.0 0.0 539.0\n",
      " sampled_center min/max: 9.99999901978299e-05 2.133143424987793\n",
      " valid_count: 1555200\n",
      "  src=0: loss_pair=139971.0312, valid_pixels=1555200\n",
      "[reproj debug SAFE] src=2 B=3 H=540 W=960\n",
      " X_src_world minZ: -315738.59375\n",
      " X_center_world_reproj minZ: 360.4757385253906\n",
      " uv min/max: 0.0 959.0 0.0 539.0\n",
      " sampled_center min/max: 9.99999901978299e-05 2.151540517807007\n",
      " valid_count: 1555200\n",
      "  src=2: loss_pair=154948.0312, valid_pixels=1555200\n",
      "[MV Loss] 147459.5312\n",
      "[Affine applied to pred] s=[-2.861518144607544, -1.1296466588974, -2.2522428035736084], t=[1.9104925394058228, 0.789181649684906, 1.5451215505599976]\n",
      "  pred_aligned min/max: -4.2476, 1.9102\n",
      "  gt_center min/max: 0.0386, inf\n",
      "[L_gt] 0.7484 (Huber, affine-aligned)\n",
      "[reproj debug SAFE] src=2 B=3 H=540 W=960\n",
      " X_src_world minZ: -315738.59375\n",
      " X_center_world_reproj minZ: 360.4757385253906\n",
      " uv min/max: 0.0 959.0 0.0 539.0\n",
      " sampled_center min/max: 9.99999901978299e-05 2.151540517807007\n",
      " valid_count: 1555200\n",
      "  src=2: loss_pair=154948.0312, valid_pixels=1555200\n",
      "[MV Loss] 147459.5312\n",
      "[Affine applied to pred] s=[-2.861518144607544, -1.1296466588974, -2.2522428035736084], t=[1.9104925394058228, 0.789181649684906, 1.5451215505599976]\n",
      "  pred_aligned min/max: -4.2476, 1.9102\n",
      "  gt_center min/max: 0.0386, inf\n",
      "[L_gt] 0.7484 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0058\n",
      "[L_edge] 0.0057\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 148.2088 (mv_weighted=147.4595)\n",
      "[L_init (edge_smooth)] 0.0058\n",
      "[L_edge] 0.0057\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 148.2088 (mv_weighted=147.4595)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 1] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 1] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=133375000.00\n",
      "[GT mask] valid_pixels=1280348 / 1555200\n",
      "[Affine align] scales=['-0.76', '-0.69', '-0.32'], offsets=['0.53', '0.52', '0.33']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=133375000.00\n",
      "[GT mask] valid_pixels=1280348 / 1555200\n",
      "[Affine align] scales=['-0.76', '-0.69', '-0.32'], offsets=['0.53', '0.52', '0.33']\n",
      "[Prediction] pred_depth min=0.0001, max=0.7721\n",
      "[Prediction] pred_depth min=0.0001, max=0.7721\n",
      "  src=0: loss_pair=147355.9219, valid_pixels=1555200\n",
      "  src=0: loss_pair=147355.9219, valid_pixels=1555200\n",
      "  src=2: loss_pair=176731.5469, valid_pixels=1555200\n",
      "[MV Loss] 162043.7344\n",
      "[Affine applied to pred] s=[-0.7608565092086792, -0.6903162002563477, -0.3221544623374939], t=[0.5282524824142456, 0.5156106352806091, 0.32812005281448364]\n",
      "  pred_aligned min/max: -0.0592, 0.5282\n",
      "  gt_center min/max: 0.0450, inf\n",
      "[L_gt] 0.1351 (Huber, affine-aligned)\n",
      "  src=2: loss_pair=176731.5469, valid_pixels=1555200\n",
      "[MV Loss] 162043.7344\n",
      "[Affine applied to pred] s=[-0.7608565092086792, -0.6903162002563477, -0.3221544623374939], t=[0.5282524824142456, 0.5156106352806091, 0.32812005281448364]\n",
      "  pred_aligned min/max: -0.0592, 0.5282\n",
      "  gt_center min/max: 0.0450, inf\n",
      "[L_gt] 0.1351 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0023\n",
      "[L_edge] 0.0033\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 162.1793 (mv_weighted=162.0437)\n",
      "[L_init (edge_smooth)] 0.0023\n",
      "[L_edge] 0.0033\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 162.1793 (mv_weighted=162.0437)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 2] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 2] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=324.21\n",
      "[GT mask] valid_pixels=1224252 / 1555200\n",
      "[Affine align] scales=['-1.94', '-1.67', '-1.72'], offsets=['1.34', '1.14', '1.21']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=324.21\n",
      "[GT mask] valid_pixels=1224252 / 1555200\n",
      "[Affine align] scales=['-1.94', '-1.67', '-1.72'], offsets=['1.34', '1.14', '1.21']\n",
      "[Prediction] pred_depth min=0.0001, max=1.5780\n",
      "[Prediction] pred_depth min=0.0001, max=1.5780\n",
      "  src=0: loss_pair=220954.5938, valid_pixels=1555200\n",
      "  src=0: loss_pair=220954.5938, valid_pixels=1555200\n",
      "  src=2: loss_pair=212801.3594, valid_pixels=1555200\n",
      "[MV Loss] 216877.9688\n",
      "[Affine applied to pred] s=[-1.9383604526519775, -1.6706292629241943, -1.7228150367736816], t=[1.3377605676651, 1.1385877132415771, 1.2053132057189941]\n",
      "  pred_aligned min/max: -1.7210, 1.3376\n",
      "  gt_center min/max: 0.0606, inf\n",
      "[L_gt] 0.6635 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0051\n",
      "  src=2: loss_pair=212801.3594, valid_pixels=1555200\n",
      "[MV Loss] 216877.9688\n",
      "[Affine applied to pred] s=[-1.9383604526519775, -1.6706292629241943, -1.7228150367736816], t=[1.3377605676651, 1.1385877132415771, 1.2053132057189941]\n",
      "  pred_aligned min/max: -1.7210, 1.3376\n",
      "  gt_center min/max: 0.0606, inf\n",
      "[L_gt] 0.6635 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0051\n",
      "[L_edge] 0.0055\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 217.5422 (mv_weighted=216.8780)\n",
      "[L_edge] 0.0055\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 217.5422 (mv_weighted=216.8780)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n",
      "\n",
      "[Step 3] B=3, V=5, H=540, W=960\n",
      "\n",
      "[Step 3] B=3, V=5, H=540, W=960\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=84.05\n",
      "[GT mask] valid_pixels=1389492 / 1555200\n",
      "[Affine align] scales=['-0.84', '-1.23', '-2.81'], offsets=['0.67', '0.83', '1.88']\n",
      "[DepthAnything] min=0.0001, max=1.0000\n",
      "[Unit detection] scale=0.0010, did_scale=True, med_scale=84.05\n",
      "[GT mask] valid_pixels=1389492 / 1555200\n",
      "[Affine align] scales=['-0.84', '-1.23', '-2.81'], offsets=['0.67', '0.83', '1.88']\n",
      "[Prediction] pred_depth min=0.0001, max=2.1267\n",
      "[Prediction] pred_depth min=0.0001, max=2.1267\n",
      "  src=0: loss_pair=73171.1562, valid_pixels=1555200\n",
      "  src=0: loss_pair=73171.1562, valid_pixels=1555200\n",
      "  src=2: loss_pair=106550.9844, valid_pixels=1555200\n",
      "[MV Loss] 89861.0703\n",
      "[Affine applied to pred] s=[-0.843437671661377, -1.2255926132202148, -2.8127923011779785], t=[0.6704275012016296, 0.83461993932724, 1.882707953453064]\n",
      "  pred_aligned min/max: -4.0991, 1.8824\n",
      "  gt_center min/max: 0.0387, inf\n",
      "[L_gt] 0.6352 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0036\n",
      "[L_edge] 0.0041\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 90.4968 (mv_weighted=89.8611)\n",
      "  src=2: loss_pair=106550.9844, valid_pixels=1555200\n",
      "[MV Loss] 89861.0703\n",
      "[Affine applied to pred] s=[-0.843437671661377, -1.2255926132202148, -2.8127923011779785], t=[0.6704275012016296, 0.83461993932724, 1.882707953453064]\n",
      "  pred_aligned min/max: -4.0991, 1.8824\n",
      "  gt_center min/max: 0.0387, inf\n",
      "[L_gt] 0.6352 (Huber, affine-aligned)\n",
      "[L_init (edge_smooth)] 0.0036\n",
      "[L_edge] 0.0041\n",
      "[L_norm] 0.0000\n",
      "[Total Loss] 90.4968 (mv_weighted=89.8611)\n",
      "[Backward] completed, gradients updated\n",
      "[Backward] completed, gradients updated\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load Depth Anything Small (if not loaded)\n",
    "# ============================================\n",
    "if 'depth_anything' not in dir():\n",
    "    from transformers import pipeline\n",
    "    print(\"[Loading] Depth Anything Small model...\")\n",
    "    depth_anything = pipeline(task=\"depth-estimation\", model=\"LiheYoung/depth-anything-small-hf\")\n",
    "    print(\"[Loaded] Depth Anything Small\")\n",
    "else:\n",
    "    print(\"[Using] Existing Depth Anything pipeline\")\n",
    "\n",
    "# ============================================\n",
    "# Example: run training with Depth Anything Small\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"[Device] Using: {device}\")\n",
    "    \n",
    "    # Initialize model (fresh)\n",
    "    model = Unet(in_channel=7, out_channel=1, base=32).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Train for 5 epochs\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Starting training with Depth Anything Small model - 5 epochs\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPOCH {epoch+1}/5\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        train_one_epoch(\n",
    "            model=model,\n",
    "            depth_anything_model=depth_anything,\n",
    "            dataloader=dataloader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            use_gt_for_loss=True,\n",
    "            lambda_mv=0.001,  # Reduce MV loss weight for balance\n",
    "            lambda_init=0.1,\n",
    "            lambda_edge=0.05,\n",
    "            lambda_norm=0.1,\n",
    "            num_steps=5  # 5 batches per epoch\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Training finished!\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "550531fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine test SUCCESS: s=[-0.3661380410194397, 0.516023576259613], t=[22.120376586914062, 23.312768936157227]\n",
      "  init range: [0.0012, 0.4991]\n",
      "  gt range: [0.0719, 49.7345]\n",
      "  aligned range: [21.9399, 23.5703]\n"
     ]
    }
   ],
   "source": [
    "# Quick test of affine align function\n",
    "test_init = torch.rand(2, 1, 10, 10) * 0.5  # [0, 0.5]\n",
    "test_gt = torch.rand(2, 1, 10, 10) * 50.0  # [0, 50]\n",
    "test_mask = torch.ones(2, 1, 10, 10, dtype=torch.bool)\n",
    "\n",
    "try:\n",
    "    aligned, s, t = affine_align_depth(test_init, test_gt, test_mask)\n",
    "    print(f\"Affine test SUCCESS: s={s}, t={t}\")\n",
    "    print(f\"  init range: [{test_init.min():.4f}, {test_init.max():.4f}]\")\n",
    "    print(f\"  gt range: [{test_gt.min():.4f}, {test_gt.max():.4f}]\")\n",
    "    print(f\"  aligned range: [{aligned.min():.4f}, {aligned.max():.4f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"Affine test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23047b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
